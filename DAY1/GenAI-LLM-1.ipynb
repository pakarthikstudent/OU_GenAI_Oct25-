{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c195d13-76ed-4662-903b-de84ac3f6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI Types\n",
    "==========\n",
    "Narrow AI - Chatbot,Google Assistant\n",
    "General AI - \n",
    "Super AI -\n",
    "    \n",
    "AI Functionality\n",
    "================\n",
    "Reactive Machines - Responds to input without memory   Ex: Chess-playing \n",
    "Limited Memory  -  Learn from past data to imporve    Ex: self driving cars\n",
    "Theory of Mind - Understand emotions and social interactions\n",
    "\n",
    "\n",
    "1. Collect the data \n",
    "2. Train Model\n",
    "3. Test Model\n",
    "4. Deploy\n",
    "5. Improve \n",
    "==============================================================\n",
    "Data \n",
    "-----\n",
    " |->Text Audio Video Image or any unstructured data ...\n",
    "\n",
    "DataAnalysis\n",
    "==============\n",
    "1. Collect the data -->2. Process Data  -------> 3. EDA  4. Visualization  ===> Model(MachineLearning)\n",
    "                       |->clean up                              \n",
    "                           |->omit duplicate values\n",
    "                           |->replace empty values\n",
    "                           |->drop unwanted columns\n",
    "                           |->drop outliers \n",
    "numpy\n",
    "pandas \n",
    "matplotlib \n",
    "\n",
    "MachineLearning(ML)\n",
    "====================\n",
    "General Program:\n",
    "---------------\n",
    "Programmer:   Input ---->[M/C]-->output\n",
    "Vs\n",
    "ML programming: Input and Output --->[M/C] -->Algorithm(model)\n",
    "\n",
    "1. Collect Data  <== from DataAnalysis \n",
    "2. Prepare\n",
    "3. Select the model \n",
    "4. Train the model\n",
    "5. Test \n",
    "6. Deploy \n",
    "7. Monitor \n",
    "\n",
    "Supervised Learning\n",
    "|->labeled data(input + correct answer)\n",
    "Liner Regression,Decision Tree,...\n",
    "\n",
    "Unsupervised Learning\n",
    "|->no label \n",
    "\n",
    "Reinforcement Learning\n",
    "|->trial and error (ex: sef-driving cars)\n",
    "Graph Machine Learning (GML)\n",
    "-----------------------------\n",
    "  |-> Graph       (pA)--------(pB)\n",
    "       G=(V,E)                 |\n",
    "                      (pC) ----|\n",
    "----------------------------------------------------------\n",
    "\n",
    "Deep Learning Works\n",
    "----------------------\n",
    "1. Collect the data (ex: images,sounds,audios,...)\n",
    "2. Neual Network \n",
    "3. Hidden layers (processing) ...\n",
    "4. Output layers \n",
    "\n",
    "ANN\n",
    "CNN \n",
    "RNN(Recurrent Neual Network) Language Transalation , Speech recognition \n",
    "|\n",
    "Transformer Network  -> NLP,Summarization...\n",
    "====================================================================================\n",
    "GenerativeAI (Gen AI)\n",
    "    |\n",
    "    Create new content(text,image,video,audio..)\n",
    "    LLM ( Text )\n",
    "                       \n",
    "Gen AI LLM - Text - ChatGPT\n",
    "           - Image - DALL-E\n",
    "\n",
    "\n",
    "EndUser: Get list of sales emp's records ? (user input)\n",
    "         -------------------------------\n",
    "                    |                |\n",
    "                    LLM <== pre-trainined ML,DL models \n",
    "                    |  \n",
    "                    VectorDataBase(oracle23 ai,FAISS,Chroma,..)\n",
    "                    ---------------// select *from emp where edept = sales //SQL\n",
    "                                        |\n",
    "Gen AI => Learn from existing data to create new,original content//\n",
    "-------\n",
    "LLM ==> predict the next word in sentence, given all the previous words\n",
    "---\n",
    "===========================================================================\n",
    "Agentic AI - AI Agents\n",
    "                 |->think,plan,take action,use tools \n",
    "\n",
    "Query: travel date -> to delhi  \n",
    "ChatGPT -> .......... travel modes ..\n",
    "                       Vs\n",
    "Query: travel date -> to delhi  \n",
    "      ....    .... visiting places + hotels + ....                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477151e-a797-41f5-a21c-75f9b93b2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP\n",
    "----\n",
    "1. Text preprocessing\n",
    "   - Tokenization\n",
    "   - lemmatization\n",
    "   - stop words\n",
    "2. Text processing\n",
    "   - Bagofwords\n",
    "   - ngram\n",
    "   - TF-IDF \n",
    "3. Text processing\n",
    "    - word2vector \n",
    "    - Deeplearning techniques \n",
    "       ()  () ()\n",
    "4.  transformer \n",
    "5. BERT \n",
    "-----------------------------\n",
    "2002 -> Supermarket Billing (VB,MSAccess) => Desktop Application\n",
    "         ====================\n",
    "2025 -> ..... pre-trained model + records = own RAG ->chatbot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4166979-4d7d-4c62-87f1-eb7fce68b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Python\n",
    "=======\n",
    " |-> 1. procedure style code = direct approach ->  var = 10 <== direct initialization\n",
    "                                                   def fx():\n",
    "                                                        ..\n",
    "                                                   fx() <== direct call\n",
    "\n",
    " |-> 2. object oriented style code => class<-->object\n",
    "                                                    obj1.var = 10 <== object based initialization\n",
    "                                                    obj2.var = 10 <== object based initialization\n",
    "                                                    obj1.fx() <== methodCall\n",
    "\n",
    " |-> 3. Functional style code => Expression (or) SinglelineCode - computing/analysis.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55ebc6-c150-413c-b2e8-e27938a8ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "In python everything is an object\n",
    "         --------------------------\n",
    "number - int,float,complex\n",
    "a-zA-Z0-9specialchars - str \n",
    "int float complex str bytes bool NoneType \n",
    "list [ ]\n",
    "tuple ( )\n",
    "dict {key:Value,key1:value,Key2:value...}\n",
    "set \n",
    "------------------------------------//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64eb20d-b3ce-4b5e-babb-99f3da623dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname='emp.csv'\n",
    "findx=123\n",
    "fsize='5KB'\n",
    "futil = 98.54\n",
    "fstatus = True\n",
    "\n",
    "fileInfo1 = ['emp.csv',123,'5KB',98.54,True] <== list\n",
    "\n",
    "fileInfo2 = ('emp.csv',123,'5KB',98.54,True) <== tuple\n",
    "\n",
    "fileInfo3 = {'K1':'emp.csv','K2':123,'K3':'5KB','K4':98.54,'K5':True} <== dict\n",
    "\n",
    "class classname:\n",
    "    attribute \n",
    "\n",
    "python user defined class - mutable - using classname - we can add new attribute\n",
    "                                                        we can modify an existing attribute\n",
    "                                                        we can delete an existing attribute\n",
    "- template (or) blue print of an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56cdd86-f4d1-4318-bc55-fdd5c7a4e2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.fileInfo'>\n"
     ]
    }
   ],
   "source": [
    "class fileInfo:\n",
    "    fname=''\n",
    "    findex=0\n",
    "print(fileInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d20c2e-e160-40e5-9d4f-69062a66d7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.fileInfo object at 0x000002993696CEC0>\n"
     ]
    }
   ],
   "source": [
    "print(fileInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1aa202-2737-4f92-9f8c-a43224b4b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1 = fileInfo()\n",
    "obj2 = fileInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed7698a-37da-4bf0-9bec-775de7aae7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.fileInfo'> <class '__main__.fileInfo'>\n"
     ]
    }
   ],
   "source": [
    "print(type(obj1),type(obj2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0469f17-5455-432d-b30e-7f69cbdcdfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(10),type(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b95cbcc-8eaa-4eef-9b0f-355ff9fc7450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.fileInfo at 0x2993677e5d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa9e6e64-e112-40c2-b918-f8841add626c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.fileInfo at 0x2993677e710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826149c3-56ae-48ee-9cb1-8d0117d82345",
   "metadata": {},
   "outputs": [],
   "source": [
    " +------------+\n",
    " | [] (white) |   <== blueprint/sheet  -- Class\n",
    " +------------+\n",
    "  |           |_________\n",
    "+------------+         +------------+\n",
    "| [] (white) |         | [] (white) |\n",
    "+------------+         +------------+\n",
    "  1st main                 2nd main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0898f587-90bc-4bac-933f-e4057f2af4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emp.csv 101\n"
     ]
    }
   ],
   "source": [
    "class fileInfo:\n",
    "    fname=''\n",
    "    findex=0\n",
    "\n",
    "obj1 = fileInfo()\n",
    "obj1.fname = \"emp.csv\"\n",
    "obj1.findex = 101\n",
    "\n",
    "obj2 = fileInfo()\n",
    "obj2.fname = \"prod.log\"\n",
    "obj2.findex = 102\n",
    "print(obj1.fname,obj1.findex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5702467-1137-4765-bfba-c741211506b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prod.log 102\n"
     ]
    }
   ],
   "source": [
    "print(obj2.fname,obj2.findex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb14be3a-1c39-49cc-8629-3828a6fae96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110, 120, 130, 140, 150]\n"
     ]
    }
   ],
   "source": [
    "L=[]\n",
    "\n",
    "def f1(a):\n",
    "    return a+100\n",
    "\n",
    "for var in [10,20,30,40,50]:\n",
    "    r = f1(var)\n",
    "    L.append(r)\n",
    "    \n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb2781c-434b-412d-b80a-654a1c587e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110, 120, 130, 140, 150]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda a:a+100,[10,20,30,40,50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c703058e-cf51-4cf2-9a69-d83a795ab758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K1': [110, 120, 130, 140, 150]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d={}\n",
    "d['K1'] = list(map(lambda a:a+100,[10,20,30,40,50]))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c16fc-60e2-4675-ad20-9b7b37ecf132",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. lambda\n",
    "---------\n",
    "lambda - python keyword - function call with args and return some value\n",
    "lambda <list of args>:<basicOperation>\n",
    "         .........      ........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bae881c-c805-4841-98e8-f08660254f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(a1,a2):\n",
    "    return a1+a2\n",
    "\n",
    "f1(10,20) # named function call with arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7814449-8482-47b6-bb53-979cad24d73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = lambda a1,a2:a1+a2\n",
    "f2(10,20) # unnamed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bca78d13-2c11-44d2-9de2-ba362b9305ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110, 120, 130, 140, 150]\n"
     ]
    }
   ],
   "source": [
    "# list comprehesion = list append operation\n",
    "L=[]\n",
    "\n",
    "for var in [10,20,30,40,50]:\n",
    "    r = var+100\n",
    "    L.append(r)\n",
    "    \n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74e3813c-d76c-4dd3-84df-8fc189e7618f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110, 120, 130, 140, 150]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[var+100 for var in [10,20,30,40,50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0ffd50d-fa6e-4b28-979a-58bdf95c31f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'> <class 'function'>\n"
     ]
    }
   ],
   "source": [
    "# Generator - function returns an address(iterator)\n",
    "# ---------   -------- ========= yield ===========\n",
    "\n",
    "def fA():\n",
    "    return 10\n",
    "\n",
    "def fB():\n",
    "    yield 10\n",
    "\n",
    "print(type(fA),type(fB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5da03e0e-11f5-4f67-a91c-5838390de89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "print(type(fA()))\n",
    "print(type(fB()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38571338-0573-47e0-8b1c-56974607cc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object fB at 0x0000029938D1F7F0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4cdf80-8924-4c82-a2d5-c3108c077e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. next(genObject) ... StopIteration\n",
    "2. for loop => for var in genObj:\n",
    "                      ..\n",
    "3. type cast to list => list(genObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a48bb9f-c159-45c6-b310-21c90011b041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e81ac-57e5-4eaa-b424-4da2e65d1ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "map(function,collection) ->genObject\n",
    "     |          |                |-->typecast to list() ; for loop\n",
    "  lambda    comprehension/inputCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1dae7f5-0db8-4351-b3ee-8083f0585917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110, 120, 130, 140, 150]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda a:a+100,[10,20,30,40,50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974549e-1e3b-4745-8404-a5ff6f73bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c73c94-7d5e-4bbb-9cf4-d06269291529",
   "metadata": {},
   "outputs": [],
   "source": [
    "python.org\n",
    "------------\n",
    " we can do general python program\n",
    " not ML DL llm DA..\n",
    "pip install numpy\n",
    "pip install pandas\n",
    "...\n",
    "\n",
    "anaconda.com/downloads \n",
    " |->python + ML libs available\n",
    "==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d157c27-a17e-4b38-bc75-a16b2ed2b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of breaking text into smaller pieces (token)\n",
    "token - word / subwords/ characters\n",
    "-----\n",
    " |->Convert to numbers (vector) =>for model understand \n",
    "\n",
    "Hello Good Morning          ..................\n",
    "(English) ------------------>(French..)\n",
    "\n",
    "hello  -> [101030]   \n",
    "good   -> [101010]\n",
    "morning -> [101033]\n",
    "\n",
    "\"I likes to read machine learning algorithm\"\n",
    "word-level => [\"I\", \"likes\", \"to\",\"read\",\"machine\",\"learning\",\"algorithm\"]\n",
    "subword-leve =>[\"I\", \"likes\", \"to\",\"read\",\"machine\", \"learn\",\"ing\",\"algo\",\"rithm\"]\n",
    "\n",
    "NLP terms\n",
    "-----------\n",
    "1. Corpus  - paragraph \n",
    "2. Documents - sentences\n",
    "3. vocabulary - unique words present in the paragraph\n",
    "\n",
    "nltk tool kit <== import nltk\n",
    "download model \n",
    "apply word/sentence///"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "329b1f5f-51a3-40bc-a8d1-1ec7f86a598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "048b530a-602c-4144-87a3-952e6ff16a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'likes', 'to', 'read', 'machine', 'learning', 'algorithm']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I likes to read machine learning algorithm\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ebdb9-cdb1-4387-a46c-ba19ce637535",
   "metadata": {},
   "outputs": [],
   "source": [
    "LookupError\n",
    ">>> import nltk\n",
    ">>> nltk.download('punkt_tab')\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03140a-8299-4e53-8a7d-b5707266a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed6639-6d94-4718-b056-88f290ace596",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"hello\" # OK\n",
    "\"hello' # Error \n",
    "'''....\n",
    "   .....'''\n",
    "\"\"\"\n",
    " ....\n",
    " .... \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ad1733d-4cc1-428d-9454-86e097b17336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Gen',\n",
       " 'AI',\n",
       " 'NLP',\n",
       " 'lecture',\n",
       " 'pleas',\n",
       " 'do',\n",
       " 'activity',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expect',\n",
       " 'in',\n",
       " 'NLP']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = '''Hello welcome,to Gen AI NLP lecture\n",
    "pleas do activity ! to become expect in NLP'''\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26cb4cd7-a055-4574-bc51-7a84cecacbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello welcome,to Gen AI NLP lecture\\npleas do activity !',\n",
       " 'to become expect in NLP']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5db8827-474f-4d66-b100-a6745a9b145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg=\"Hello arun,How are you doing ? OK. I am doing NLP activitites's code ab'c data1.data2 #!/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2908dffa-261a-4bee-ada2-2f49ee8a3d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'arun',\n",
       " ',',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'OK',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'doing',\n",
       " 'NLP',\n",
       " 'activitites',\n",
       " \"'s\",\n",
       " 'code',\n",
       " 'ab',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'data1.data2',\n",
       " '#',\n",
       " '!',\n",
       " '/bin']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd35eab4-c960-4beb-a1b9-c833db788ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'arun',\n",
       " ',',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'OK',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'doing',\n",
       " 'NLP',\n",
       " 'activitites',\n",
       " \"'\",\n",
       " 's',\n",
       " 'code',\n",
       " 'ab',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'data1',\n",
       " '.',\n",
       " 'data2',\n",
       " '#!/',\n",
       " 'bin']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7cfcdc6-3fa2-47b7-9417-bf8c91dcf7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tokenize.treebank.TreebankWordTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "print(TreebankWordTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc900c24-c5bc-4c3b-88b1-dd094dc56b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\nltk\\\\__init__.py'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca028bc6-aecc-466c-9d90-c46599297ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "<class 'nltk.tokenize.treebank.TreebankWordTokenizer'>\n",
    "nltk/\n",
    "  |__tokenize/\n",
    "            |__treebank.py\n",
    "                    |\n",
    "                    |->class TreebankWordTokenizer:\n",
    "                                 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d43b51c0-577d-4cab-bead-53af2810426d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'do',\n",
       " 'this',\n",
       " ',',\n",
       " 'can',\n",
       " 'i',\n",
       " '?',\n",
       " \"ab'c\",\n",
       " \"def'g\",\n",
       " 'hi',\n",
       " ':',\n",
       " 'j']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg=\"I can't do this, can i? ab'c def'g hi:j\"\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "obj = TreebankWordTokenizer()\n",
    "obj.tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7c45679-64d8-42c1-99bb-40e82ba16cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"They'll test-drive the car tomorrow.\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize,TreebankWordTokenizer,WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65e35bef-f15f-4ae6-803b-b0e9c903d473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', \"'ll\", 'test-drive', 'the', 'car', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56741ef3-a7ea-4ca4-9ed7-6dc43fe1a092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', \"'ll\", 'test-drive', 'the', 'car', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "print(TreebankWordTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "127a77e4-f9d8-488b-a11a-ebed2b307fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', \"'\", 'll', 'test', '-', 'drive', 'the', 'car', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee1be3f3-6719-424b-b1d8-434ac2e69f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## stemming - stem => reduce the root word \n",
    "from nltk.stem import PorterStemmer\n",
    "obj_stemming = PorterStemmer()\n",
    "obj_stemming.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5677c4c2-5e1b-4a0b-9016-3cce09211d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_stemming.stem(\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f49f6bee-68ec-45eb-80f4-4a1a176e9474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_stemming.stem(\"learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12105904-c80e-46c4-80fc-4348f842c1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_stemming.stem(\"congratulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78668ca3-3577-4745-b593-0d65dafcbd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regex expression\n",
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemm = RegexpStemmer(\"ing$|s$|e$\")\n",
    "reg_stemm.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75e8c085-7fd3-4cb3-975f-fcd5f8ed47c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemm.stem(\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4d7bbf4-e01f-425d-8d55-198ecc7f0367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemm.stem(\"abcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21644fe6-6e01-4a50-9e3e-e473c70e6c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemm.stem(\"abcde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3393a402-ce80-4c71-95e5-40c8a3a7a048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization - lemma \n",
    "# reduce root word - based on pos='v' ''\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatize_obj = WordNetLemmatizer()\n",
    "lemmatize_obj.lemmatize(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "334cdb1c-0d30-4fdf-b5c2-95a9cf022c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_obj.lemmatize(\"eating\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c026a5b7-3804-4e85-9ddc-1c440c445e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_obj.lemmatize(\"history\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfe27533-3a85-43b3-b2b5-ac8274a3d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(lemmatize_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735427d7-71df-4ce6-a7bd-304f540b105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "the hotel food is good  => hotel food good => [123,442,552] ->DB ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17778fa6-32de-47c0-a5dd-61963f5a6a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a721ebe1-f47f-494e-8bba-0f1570c4029b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')\n",
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eea75ff3-eac4-4419-b501-32296809e97b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['albanian',\n",
       " 'arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'belarusian',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'tamil',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get list stopwords from other language \n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3839f8d6-2ce3-4f2e-ab7b-b5019510be6e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'أبٌ',\n",
       " 'أخٌ',\n",
       " 'حمٌ',\n",
       " 'فو',\n",
       " 'أنتِ',\n",
       " 'يناير',\n",
       " 'فبراير',\n",
       " 'مارس',\n",
       " 'أبريل',\n",
       " 'مايو',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'سبتمبر',\n",
       " 'أكتوبر',\n",
       " 'نوفمبر',\n",
       " 'ديسمبر',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'مارس',\n",
       " 'أفريل',\n",
       " 'ماي',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'أوت',\n",
       " 'كانون',\n",
       " 'شباط',\n",
       " 'آذار',\n",
       " 'نيسان',\n",
       " 'أيار',\n",
       " 'حزيران',\n",
       " 'تموز',\n",
       " 'آب',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'ريال',\n",
       " 'درهم',\n",
       " 'ليرة',\n",
       " 'جنيه',\n",
       " 'قرش',\n",
       " 'مليم',\n",
       " 'فلس',\n",
       " 'هللة',\n",
       " 'سنتيم',\n",
       " 'يورو',\n",
       " 'ين',\n",
       " 'يوان',\n",
       " 'شيكل',\n",
       " 'واحد',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'خمسة',\n",
       " 'ستة',\n",
       " 'سبعة',\n",
       " 'ثمانية',\n",
       " 'تسعة',\n",
       " 'عشرة',\n",
       " 'أحد',\n",
       " 'اثنا',\n",
       " 'اثني',\n",
       " 'إحدى',\n",
       " 'ثلاث',\n",
       " 'أربع',\n",
       " 'خمس',\n",
       " 'ست',\n",
       " 'سبع',\n",
       " 'ثماني',\n",
       " 'تسع',\n",
       " 'عشر',\n",
       " 'ثمان',\n",
       " 'سبت',\n",
       " 'أحد',\n",
       " 'اثنين',\n",
       " 'ثلاثاء',\n",
       " 'أربعاء',\n",
       " 'خميس',\n",
       " 'جمعة',\n",
       " 'أول',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثالث',\n",
       " 'رابع',\n",
       " 'خامس',\n",
       " 'سادس',\n",
       " 'سابع',\n",
       " 'ثامن',\n",
       " 'تاسع',\n",
       " 'عاشر',\n",
       " 'حادي',\n",
       " 'أ',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ء',\n",
       " 'ى',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'أ',\n",
       " 'ة',\n",
       " 'ألف',\n",
       " 'باء',\n",
       " 'تاء',\n",
       " 'ثاء',\n",
       " 'جيم',\n",
       " 'حاء',\n",
       " 'خاء',\n",
       " 'دال',\n",
       " 'ذال',\n",
       " 'راء',\n",
       " 'زاي',\n",
       " 'سين',\n",
       " 'شين',\n",
       " 'صاد',\n",
       " 'ضاد',\n",
       " 'طاء',\n",
       " 'ظاء',\n",
       " 'عين',\n",
       " 'غين',\n",
       " 'فاء',\n",
       " 'قاف',\n",
       " 'كاف',\n",
       " 'لام',\n",
       " 'ميم',\n",
       " 'نون',\n",
       " 'هاء',\n",
       " 'واو',\n",
       " 'ياء',\n",
       " 'همزة',\n",
       " 'ي',\n",
       " 'نا',\n",
       " 'ك',\n",
       " 'كن',\n",
       " 'ه',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهما',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياك',\n",
       " 'إياكما',\n",
       " 'إياكم',\n",
       " 'إياك',\n",
       " 'إياكن',\n",
       " 'إياي',\n",
       " 'إيانا',\n",
       " 'أولالك',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'تَيْنِ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ذانِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ذَيْنِ',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَذَيْنِ',\n",
       " 'الألى',\n",
       " 'الألاء',\n",
       " 'أل',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'ذيت',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'بضع',\n",
       " 'فلان',\n",
       " 'وا',\n",
       " 'آمينَ',\n",
       " 'آهِ',\n",
       " 'آهٍ',\n",
       " 'آهاً',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أفٍّ',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أوّهْ',\n",
       " 'إلَيْكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إليكَ',\n",
       " 'إليكنّ',\n",
       " 'إيهٍ',\n",
       " 'بخٍ',\n",
       " 'بسّ',\n",
       " 'بَسْ',\n",
       " 'بطآن',\n",
       " 'بَلْهَ',\n",
       " 'حاي',\n",
       " 'حَذارِ',\n",
       " 'حيَّ',\n",
       " 'حيَّ',\n",
       " 'دونك',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'شَتَّانَ',\n",
       " 'صهْ',\n",
       " 'صهٍ',\n",
       " 'طاق',\n",
       " 'طَق',\n",
       " 'عَدَسْ',\n",
       " 'كِخ',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'نَخْ',\n",
       " 'هاكَ',\n",
       " 'هَجْ',\n",
       " 'هلم',\n",
       " 'هيّا',\n",
       " 'هَيْهات',\n",
       " 'وا',\n",
       " 'واهاً',\n",
       " 'وراءَك',\n",
       " 'وُشْكَانَ',\n",
       " 'وَيْ',\n",
       " 'يفعلان',\n",
       " 'تفعلان',\n",
       " 'يفعلون',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'اتخذ',\n",
       " 'ألفى',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تعلَّم',\n",
       " 'جعل',\n",
       " 'حجا',\n",
       " 'حبيب',\n",
       " 'خال',\n",
       " 'حسب',\n",
       " 'خال',\n",
       " 'درى',\n",
       " 'رأى',\n",
       " 'زعم',\n",
       " 'صبر',\n",
       " 'ظنَّ',\n",
       " 'عدَّ',\n",
       " 'علم',\n",
       " 'غادر',\n",
       " 'ذهب',\n",
       " 'وجد',\n",
       " 'ورد',\n",
       " 'وهب',\n",
       " 'أسكن',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'رزق',\n",
       " 'زود',\n",
       " 'سقى',\n",
       " 'كسا',\n",
       " 'أخبر',\n",
       " 'أرى',\n",
       " 'أعلم',\n",
       " 'أنبأ',\n",
       " 'حدَث',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'أفعل به',\n",
       " 'ما أفعله',\n",
       " 'بئس',\n",
       " 'ساء',\n",
       " 'طالما',\n",
       " 'قلما',\n",
       " 'لات',\n",
       " 'لكنَّ',\n",
       " 'ءَ',\n",
       " 'أجل',\n",
       " 'إذاً',\n",
       " 'أمّا',\n",
       " 'إمّا',\n",
       " 'إنَّ',\n",
       " 'أنًّ',\n",
       " 'أى',\n",
       " 'إى',\n",
       " 'أيا',\n",
       " 'ب',\n",
       " 'ثمَّ',\n",
       " 'جلل',\n",
       " 'جير',\n",
       " 'رُبَّ',\n",
       " 'س',\n",
       " 'علًّ',\n",
       " 'ف',\n",
       " 'كأنّ',\n",
       " 'كلَّا',\n",
       " 'كى',\n",
       " 'ل',\n",
       " 'لات',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'م',\n",
       " 'نَّ',\n",
       " 'هلّا',\n",
       " 'وا',\n",
       " 'أل',\n",
       " 'إلّا',\n",
       " 'ت',\n",
       " 'ك',\n",
       " 'لمّا',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ا',\n",
       " 'ي',\n",
       " 'تجاه',\n",
       " 'تلقاء',\n",
       " 'جميع',\n",
       " 'حسب',\n",
       " 'سبحان',\n",
       " 'شبه',\n",
       " 'لعمر',\n",
       " 'مثل',\n",
       " 'معاذ',\n",
       " 'أبو',\n",
       " 'أخو',\n",
       " 'حمو',\n",
       " 'فو',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ثلاثمئة',\n",
       " 'أربعمئة',\n",
       " 'خمسمئة',\n",
       " 'ستمئة',\n",
       " 'سبعمئة',\n",
       " 'ثمنمئة',\n",
       " 'تسعمئة',\n",
       " 'مائة',\n",
       " 'ثلاثمائة',\n",
       " 'أربعمائة',\n",
       " 'خمسمائة',\n",
       " 'ستمائة',\n",
       " 'سبعمائة',\n",
       " 'ثمانمئة',\n",
       " 'تسعمائة',\n",
       " 'عشرون',\n",
       " 'ثلاثون',\n",
       " 'اربعون',\n",
       " 'خمسون',\n",
       " 'ستون',\n",
       " 'سبعون',\n",
       " 'ثمانون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'ثلاثين',\n",
       " 'اربعين',\n",
       " 'خمسين',\n",
       " 'ستين',\n",
       " 'سبعين',\n",
       " 'ثمانين',\n",
       " 'تسعين',\n",
       " 'بضع',\n",
       " 'نيف',\n",
       " 'أجمع',\n",
       " 'جميع',\n",
       " 'عامة',\n",
       " 'عين',\n",
       " 'نفس',\n",
       " 'لا سيما',\n",
       " 'أصلا',\n",
       " 'أهلا',\n",
       " 'أيضا',\n",
       " 'بؤسا',\n",
       " 'بعدا',\n",
       " 'بغتة',\n",
       " 'تعسا',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'خلافا',\n",
       " 'خاصة',\n",
       " 'دواليك',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سمعا',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'طرا',\n",
       " 'عجبا',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'لبيك',\n",
       " 'معاذ',\n",
       " 'أبدا',\n",
       " 'إزاء',\n",
       " 'أصلا',\n",
       " 'الآن',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'آنفا',\n",
       " 'آناء',\n",
       " 'أنّى',\n",
       " 'أول',\n",
       " 'أيّان',\n",
       " 'تارة',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'حقا',\n",
       " 'صباح',\n",
       " 'مساء',\n",
       " 'ضحوة',\n",
       " 'عوض',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'قطّ',\n",
       " 'كلّما',\n",
       " 'لدن',\n",
       " 'لمّا',\n",
       " 'مرّة',\n",
       " 'قبل',\n",
       " 'خلف',\n",
       " 'أمام',\n",
       " 'فوق',\n",
       " 'تحت',\n",
       " 'يمين',\n",
       " 'شمال',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'أصبح',\n",
       " 'أضحى',\n",
       " 'آض',\n",
       " 'أمسى',\n",
       " 'انقلب',\n",
       " 'بات',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'حار',\n",
       " 'رجع',\n",
       " 'راح',\n",
       " 'صار',\n",
       " 'ظلّ',\n",
       " 'عاد',\n",
       " 'غدا',\n",
       " 'كان',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'أخذ',\n",
       " 'اخلولق',\n",
       " 'أقبل',\n",
       " 'انبرى',\n",
       " 'أنشأ',\n",
       " 'أوشك',\n",
       " 'جعل',\n",
       " 'حرى',\n",
       " 'شرع',\n",
       " 'طفق',\n",
       " 'علق',\n",
       " 'قام',\n",
       " 'كرب',\n",
       " 'كاد',\n",
       " 'هبّ']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde094b-60ed-440a-9443-9b22c8724991",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Lunch break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220b73e-549b-457c-b0db-58b9cb1e9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "Given_list=[\"words\",\"eats\",\"eatern\",\"writing\",\"writes\",\"programming\",\"program\",\"history\",\"finally\",\n",
    "            \"finalized\"]\n",
    "\n",
    "# import nltk module\n",
    "# using PorterStemmer - omit given list - root words\n",
    "# using WordNetLemmatizer - pos=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13051688-9d9f-4c06-8633-d4cfaf32d402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words --> word\n",
      "eats --> eat\n",
      "eatern --> eatern\n",
      "writing --> write\n",
      "writes --> write\n",
      "programming --> program\n",
      "program --> program\n",
      "history --> histori\n",
      "finally --> final\n",
      "finalized --> final\n"
     ]
    }
   ],
   "source": [
    "Given_list=[\"words\",\"eats\",\"eatern\",\"writing\",\"writes\",\"programming\",\"program\",\"history\",\"finally\",\n",
    "            \"finalized\"]\n",
    "\n",
    "stemming_obj = PorterStemmer()\n",
    "\n",
    "for var in Given_list:\n",
    "    print(f'{var} --> {stemming_obj.stem(var)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62fefff3-a4ba-44a7-a0e3-4c603a918434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words --> word\n",
      "eats --> eat\n",
      "eatern --> eatern\n",
      "writing --> write\n",
      "writes --> write\n",
      "programming --> program\n",
      "program --> program\n",
      "history --> history\n",
      "finally --> finally\n",
      "finalized --> finalize\n"
     ]
    }
   ],
   "source": [
    "lemmatizer_obj = WordNetLemmatizer()\n",
    "for var in Given_list:\n",
    "    print(f\"{var} --> {lemmatizer_obj.lemmatize(var,pos='v')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "01d6c7a1-82cd-4f8e-82e6-04051a2e11d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example displaying off list of or collection stop words filters\n"
     ]
    }
   ],
   "source": [
    "msg=\"This is an example displaying off list of or collection stop words filters\"\n",
    "# do wordtokenize\n",
    "# ---------------\n",
    "# stopwords.words('english')\n",
    "# ----//filter non-stop words \n",
    "# display filter words\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8c04b287-2fa8-4d60-8145-36548e1a3de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example\n",
      "displaying\n",
      "list\n",
      "collection\n",
      "stop\n",
      "words\n",
      "filters\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(msg)\n",
    "for var in words:\n",
    "    if var.lower() not in stopwords.words('english'):\n",
    "        print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39797cb3-9b5e-4263-acd3-50feb40cc502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example', 'displaying', 'list', 'collection', 'stop', 'words', 'filters']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(msg)\n",
    "filterd_words = [var for var in words if var.lower() not in stopwords.words('english')]\n",
    "filterd_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58cb1b-21c8-4e5c-b99c-8b5aa7ecb454",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task\n",
    "=====\n",
    "Corpus\n",
    "\n",
    "|->use PorterStemmer() and WordNetLemmatizer()\n",
    "|->apply stopwords\n",
    "|->filter remaning words \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f67d6798-52c2-4cd3-9480-8f4f262760de",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus='''Dr.Sarabhai is considered as the Father of the Indian space program;\n",
    "He was a great institution builder and established or helped to establish a large number of institutions \n",
    "in diverse fields.\n",
    "He was instrumental in establishing the Physical Research Laboratory (PRL) in Ahmedabad : \n",
    "after returning from Cambridge to an independent India in 1947, he persuaded charitable trusts controlled by his family and friends to endow a research institution near home in Ahmedabad. Thus, Vikram Sarabhai founded the Physical Research Laboratory (PRL) in Ahmedabad on November 11, 1947. He was only 28 at that time. Sarabhai was a creator and cultivator of institutions and PRL was the first step in that direction. Vikram Sarabhai served of PRL from 1966-1971.\n",
    "He was also Chairman of the Atomic Energy Commission.\n",
    "He along with other Ahmedabad-based industrialists played a major role in the creation \n",
    "of the Indian Institute of Management, Ahmedabad.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8205f5e0-68e4-495b-833e-614a10fe0604",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 8\n",
      "['Dr.Sarabhai is considered as the Father of the Indian space program;\\nHe was a great institution builder and established or helped to establish a large number of institutions \\nin diverse fields.', 'He was instrumental in establishing the Physical Research Laboratory (PRL) in Ahmedabad : \\nafter returning from Cambridge to an independent India in 1947, he persuaded charitable trusts controlled by his family and friends to endow a research institution near home in Ahmedabad.', 'Thus, Vikram Sarabhai founded the Physical Research Laboratory (PRL) in Ahmedabad on November 11, 1947.', 'He was only 28 at that time.', 'Sarabhai was a creator and cultivator of institutions and PRL was the first step in that direction.', 'Vikram Sarabhai served of PRL from 1966-1971.', 'He was also Chairman of the Atomic Energy Commission.', 'He along with other Ahmedabad-based industrialists played a major role in the creation \\nof the Indian Institute of Management, Ahmedabad.']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "docs = sent_tokenize(corpus)\n",
    "print(type(docs),len(docs))\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "beb7ee18-8306-4d38-b721-99b2f5e65cc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr.Sarabhai', 'is', 'considered', 'as', 'the', 'Father', 'of', 'the', 'Indian', 'space', 'program', ';', 'He', 'was', 'a', 'great', 'institution', 'builder', 'and', 'established', 'or', 'helped', 'to', 'establish', 'a', 'large', 'number', 'of', 'institutions', 'in', 'diverse', 'fields', '.']\n",
      "['He', 'was', 'instrumental', 'in', 'establishing', 'the', 'Physical', 'Research', 'Laboratory', '(', 'PRL', ')', 'in', 'Ahmedabad', ':', 'after', 'returning', 'from', 'Cambridge', 'to', 'an', 'independent', 'India', 'in', '1947', ',', 'he', 'persuaded', 'charitable', 'trusts', 'controlled', 'by', 'his', 'family', 'and', 'friends', 'to', 'endow', 'a', 'research', 'institution', 'near', 'home', 'in', 'Ahmedabad', '.']\n",
      "['Thus', ',', 'Vikram', 'Sarabhai', 'founded', 'the', 'Physical', 'Research', 'Laboratory', '(', 'PRL', ')', 'in', 'Ahmedabad', 'on', 'November', '11', ',', '1947', '.']\n",
      "['He', 'was', 'only', '28', 'at', 'that', 'time', '.']\n",
      "['Sarabhai', 'was', 'a', 'creator', 'and', 'cultivator', 'of', 'institutions', 'and', 'PRL', 'was', 'the', 'first', 'step', 'in', 'that', 'direction', '.']\n",
      "['Vikram', 'Sarabhai', 'served', 'of', 'PRL', 'from', '1966-1971', '.']\n",
      "['He', 'was', 'also', 'Chairman', 'of', 'the', 'Atomic', 'Energy', 'Commission', '.']\n",
      "['He', 'along', 'with', 'other', 'Ahmedabad-based', 'industrialists', 'played', 'a', 'major', 'role', 'in', 'the', 'creation', 'of', 'the', 'Indian', 'Institute', 'of', 'Management', ',', 'Ahmedabad', '.']\n"
     ]
    }
   ],
   "source": [
    "for var in range(len(docs)):\n",
    "    words = word_tokenize(docs[var])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5dbcc3f3-f250-4421-ac89-d4d9956614f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr.sarabhai\n",
      "considered\n",
      "father\n",
      "indian\n",
      "space\n",
      "program\n",
      ";\n",
      "great\n",
      "institution\n",
      "builder\n",
      "established\n",
      "helped\n",
      "establish\n",
      "large\n",
      "number\n",
      "institutions\n",
      "diverse\n",
      "fields\n",
      ".\n",
      "instrumental\n",
      "establishing\n",
      "physical\n",
      "research\n",
      "laboratory\n",
      "(\n",
      "prl\n",
      ")\n",
      "ahmedabad\n",
      ":\n",
      "returning\n",
      "cambridge\n",
      "independent\n",
      "india\n",
      "1947\n",
      ",\n",
      "persuaded\n",
      "charitable\n",
      "trusts\n",
      "controlled\n",
      "family\n",
      "friends\n",
      "endow\n",
      "research\n",
      "institution\n",
      "near\n",
      "home\n",
      "ahmedabad\n",
      ".\n",
      "thus\n",
      ",\n",
      "vikram\n",
      "sarabhai\n",
      "founded\n",
      "physical\n",
      "research\n",
      "laboratory\n",
      "(\n",
      "prl\n",
      ")\n",
      "ahmedabad\n",
      "november\n",
      "11\n",
      ",\n",
      "1947\n",
      ".\n",
      "28\n",
      "time\n",
      ".\n",
      "sarabhai\n",
      "creator\n",
      "cultivator\n",
      "institutions\n",
      "prl\n",
      "first\n",
      "step\n",
      "direction\n",
      ".\n",
      "vikram\n",
      "sarabhai\n",
      "served\n",
      "prl\n",
      "1966-1971\n",
      ".\n",
      "also\n",
      "chairman\n",
      "atomic\n",
      "energy\n",
      "commission\n",
      ".\n",
      "along\n",
      "ahmedabad-based\n",
      "industrialists\n",
      "played\n",
      "major\n",
      "role\n",
      "creation\n",
      "indian\n",
      "institute\n",
      "management\n",
      ",\n",
      "ahmedabad\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for var in range(len(docs)):\n",
    "    words = word_tokenize(docs[var])\n",
    "    for word in words:\n",
    "        if word.lower() not in stopwords.words('english'):\n",
    "            print(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ce76b0ab-b13c-4da8-ad61-ca58a5702906",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr.sarabhai\n",
      "considered\n",
      "father\n",
      "indian\n",
      "space\n",
      "program\n",
      ";\n",
      "great\n",
      "institution\n",
      "builder\n",
      "established\n",
      "helped\n",
      "establish\n",
      "large\n",
      "number\n",
      "institutions\n",
      "diverse\n",
      "fields\n",
      ".\n",
      "instrumental\n",
      "establishing\n",
      "physical\n",
      "research\n",
      "laboratory\n",
      "(\n",
      "prl\n",
      ")\n",
      "ahmedabad\n",
      ":\n",
      "returning\n",
      "cambridge\n",
      "independent\n",
      "india\n",
      "1947\n",
      ",\n",
      "persuaded\n",
      "charitable\n",
      "trusts\n",
      "controlled\n",
      "family\n",
      "friends\n",
      "endow\n",
      "research\n",
      "institution\n",
      "near\n",
      "home\n",
      "ahmedabad\n",
      ".\n",
      "thus\n",
      ",\n",
      "vikram\n",
      "sarabhai\n",
      "founded\n",
      "physical\n",
      "research\n",
      "laboratory\n",
      "(\n",
      "prl\n",
      ")\n",
      "ahmedabad\n",
      "november\n",
      "11\n",
      ",\n",
      "1947\n",
      ".\n",
      "28\n",
      "time\n",
      ".\n",
      "sarabhai\n",
      "creator\n",
      "cultivator\n",
      "institutions\n",
      "prl\n",
      "first\n",
      "step\n",
      "direction\n",
      ".\n",
      "vikram\n",
      "sarabhai\n",
      "served\n",
      "prl\n",
      "1966-1971\n",
      ".\n",
      "also\n",
      "chairman\n",
      "atomic\n",
      "energy\n",
      "commission\n",
      ".\n",
      "along\n",
      "ahmedabad-based\n",
      "industrialists\n",
      "played\n",
      "major\n",
      "role\n",
      "creation\n",
      "indian\n",
      "institute\n",
      "management\n",
      ",\n",
      "ahmedabad\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for var in range(len(docs)):\n",
    "    words = word_tokenize(docs[var])\n",
    "    for word in words:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            print(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ea24d434-708b-4cf9-b597-74b1360c355e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DATA1', 'Dr.sarabhai', 'data1', 'data2', 'sarabhai'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=['data1','data2','data1','data2','DATA1','Dr.sarabhai','sarabhai']\n",
    "set(L) # omit duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "98dc5fdf-48ee-4955-8ec2-ffab7c1752e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D1 D2'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=['D1','D2']\n",
    "' '.join(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5bd0db53-0444-4803-ae74-3e95aa98acfd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dr.sarabhai consid father indian space program ; he great institut builder establish help establish larg number institut divers field .',\n",
       " 'he instrument establish physic research laboratori ( prl ) ahmedabad : return cambridg independ india 1947 , persuad charit trust control famili friend endow research institut near home ahmedabad .',\n",
       " 'thu , vikram sarabhai found physic research laboratori ( prl ) ahmedabad novemb 11 , 1947 .',\n",
       " 'he 28 time .',\n",
       " 'sarabhai creator cultiv institut prl first step direct .',\n",
       " 'vikram sarabhai serv prl 1966-1971 .',\n",
       " 'he also chairman atom energi commiss .',\n",
       " 'he along ahmedabad-bas industrialist play major role creation indian institut manag , ahmedabad .']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "docs = nltk.sent_tokenize(corpus)\n",
    "\n",
    "for var in range(len(docs)):\n",
    "    words = nltk.word_tokenize(docs[var])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    docs[var] = \" \".join(words)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c12eda-4ecf-43e5-9426-0335f2240e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13761764-0988-4a36-99d8-e72912859c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a2504603-6e26-46e4-bc07-72b8a9ab0683",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.Sarabhai consider Father Indian space program ; He great institution builder establish help establish large number institutions diverse field .',\n",
       " 'He instrumental establish Physical Research Laboratory ( PRL ) Ahmedabad : return Cambridge independent India 1947 , persuade charitable trust control family friends endow research institution near home Ahmedabad .',\n",
       " 'Thus , Vikram Sarabhai found Physical Research Laboratory ( PRL ) Ahmedabad November 11 , 1947 .',\n",
       " 'He 28 time .',\n",
       " 'Sarabhai creator cultivator institutions PRL first step direction .',\n",
       " 'Vikram Sarabhai serve PRL 1966-1971 .',\n",
       " 'He also Chairman Atomic Energy Commission .',\n",
       " 'He along Ahmedabad-based industrialists play major role creation Indian Institute Management , Ahmedabad .']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = nltk.sent_tokenize(corpus)\n",
    "\n",
    "for var in range(len(docs)):\n",
    "    words = nltk.word_tokenize(docs[var])\n",
    "    words = [lemmatizer.lemmatize(word,pos='v') for word in words \n",
    "             if word not in set(stopwords.words('english'))]\n",
    "    docs[var] = \" \".join(words)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c576c-483d-49e9-a163-6d8efd1fd4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dac60b-d037-4867-9a34-7b5133caa54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams\n",
    "========\n",
    "An n-gram is contiguous sequence of n items from a given corpus/speech.\n",
    "\n",
    "food good bad <== after apply lemmatization and stop words\n",
    "-------------\n",
    " |-> [food good bad]  =>  n = 3 <= no.of unique words\n",
    "\n",
    "One-Hot encoding\n",
    "-----------------\n",
    "food             =>  1 0 0\n",
    "good             =>  0 1 0\n",
    "bad              =>  0 0 1 \n",
    "\n",
    "1. Predict next words\n",
    "This is ...... <=== next word could .. \n",
    "\n",
    "2. Text similartiy \n",
    "\n",
    "food             =>  1 0 0\n",
    "good             =>  0 1 0\n",
    "bad              =>  0 0 1 \n",
    "------------------------------ n = 1 \n",
    "\n",
    "\n",
    "food good bad\n",
    "n=2\n",
    "food good  1 0\n",
    "good bad   0 1\n",
    "\n",
    "n=3\n",
    "food good bad => 1\n",
    "                 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "40315224-0878-4f0d-9976-c28d7997dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e9d4e27-fda5-49ca-a77b-db098dbc2c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x000002993C5C9840>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"likes read nlp applications\"\n",
    "tokens = word_tokenize(data)\n",
    "ngrams(tokens,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89872130-7972-468d-9f0a-37f2fafb4387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes',), ('read',), ('nlp',), ('applications',)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a32f53cd-6166-4882-9388-ff708f61e048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes', 'read'), ('read', 'nlp'), ('nlp', 'applications')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d130a1b5-3b76-4209-9832-293eb3144140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes', 'read', 'nlp'), ('read', 'nlp', 'applications')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1a9db27c-9532-4172-afc9-9bd5a4dabbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes', 'read', 'nlp', 'applications')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f9778a28-0347-41d7-9db5-b19c1003d20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens,5))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "464c6386-aa16-4ec3-917c-91fdd009ffe7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8f41cabf-f3dd-4d8c-829d-e70b9d78723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['father', 'he', 'indian', 'program', 'regrad', 'space', 'widely'],\n",
      "      dtype='<U7')]\n",
      "\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = ['he','widely','regrad','father','indian','space','program']\n",
    "\n",
    "data = np.array(data).reshape(-1,1) # 2D\n",
    "\n",
    "encoder_obj = OneHotEncoder(sparse_output=False)\n",
    "one_hot = encoder_obj.fit_transform(data)\n",
    "print(encoder_obj.categories_)\n",
    "print('')\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f3cda745-74de-4644-900f-9c10039c3b73",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m L\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m40\u001b[39m,\u001b[38;5;241m50\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m L\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "L=[10,20,30,40,50]\n",
    "L+100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "32fe86d7-2045-4af5-8ea9-ec93e2dc23ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([110, 120, 130, 140, 150])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.array([10,20,30,40,50])\n",
    "arr+100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3ccf5219-fd51-43bf-9f66-ddb49fe7d477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1e130e71-f671-4989-a0db-ac6b9cca6adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[10,20,30,40,50]])\n",
    "arr.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "38fa7ec6-0a4f-41d3-a194-2baadff03946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 20, 30, 40, 50]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0b6fb5e0-57cc-45de-8f02-42a58aa5cea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30 40 50 60]\n"
     ]
    }
   ],
   "source": [
    "arr=np.array([10,20,30,40,50,60])\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7ccd1e3b-12d7-4a0f-a4e9-e172098045ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 20, 30],\n",
       "       [40, 50, 60]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.reshape(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e6595a69-3d0a-44a0-bdf3-9d5df06da3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 20],\n",
       "       [30, 40],\n",
       "       [50, 60]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.reshape(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b8537b0f-2672-40bd-978a-a5208739fefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed39fa98-31ce-4a23-9cab-57e828b21511",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag of Words (BOW) \n",
    "------------------\n",
    " ->word frequency \n",
    " ->not word order / grammar \n",
    "\n",
    "Bag ->contains list of items\n",
    "\n",
    "doc1 => i like nlp\n",
    "doc2 => i like machine learning\n",
    "|\n",
    "Step1 : Collect all unique words => [ 'i','like','nlp','machine','learning']\n",
    "Step2: Create vector for each sentence \n",
    "+--------------------------+-----+------+-----+---------+----------+\n",
    "| sentence                 |  i  | like | nlp | machine | learning |\n",
    "|i like nlp                |  1      1     1      0           0\n",
    "|i like machine learning   |  1      1     0      1           1\n",
    "+------------------------------------------------------------------+\n",
    "\n",
    "S1 = [ 1,1,1,0,0]\n",
    "S2 = [ 1,1,0,1,1]\n",
    "|\n",
    "similariry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5fc28e51-bbf8-4e2c-a0b2-bdc506e8b186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocs: ['learning' 'like' 'machine' 'nlp']\n",
      "\n",
      "BOW: [[0 1 0 1]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# sample docs\n",
    "text = ['i like nlp','i like machine learning']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(text)\n",
    "print('vocs:',vectorizer.get_feature_names_out())\n",
    "print('')\n",
    "print('BOW:',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3154c-1441-4d8d-a3fd-c267ddfe4beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Practice time - 5-10 mts\n",
    "https://github.com/pakarthikstudent/OU_GenAI_Oct25-/blob/main/DAY1/GenAI-LLM-1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f98043-f828-42c6-a444-b88155215e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF-IDF\n",
    "            Total terms in documents\n",
    "TF =       ---------------------------------------\n",
    "              Number of times term appears in document\n",
    "\n",
    "DF = log(TF)\n",
    "\n",
    "TF - IDF score\n",
    "TF X IDF\n",
    "weigheted  0-1\n",
    "\n",
    "Downweight = 0\n",
    "upweight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fed98f2b-1576-4de6-87d3-24fef6002e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocs: ['learning' 'like' 'machine' 'nlp']\n",
      "\n",
      "TF-IDF: [[0.         0.57973867 0.         0.81480247]\n",
      " [0.6316672  0.44943642 0.6316672  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# sample docs\n",
    "text = ['i like nlp','i like machine learning']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(text)\n",
    "print('vocs:',vectorizer.get_feature_names_out())\n",
    "print('')\n",
    "print('TF-IDF:',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75affd0f-0259-4ba8-93c1-040deb3774e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b1a07a-752b-4a1d-aa64-8e3497b86645",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW  Vs                    TF-IDF\n",
    "---\n",
    "rawcounts               weighted\n",
    "text classification     search engines\n",
    "\n",
    "TF-IDF  Vs                       word2vec\n",
    "-------                         =========\n",
    "statistical                    neuarl network\n",
    "frequence based(pattern)      semantic (pattern - meaning-based => hello hi )\n",
    "relationship between words    relationship between words applied based semantic \n",
    "not applied               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e80d0-db97-44e8-9e84-c2c09f462d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c748e0e7-32f8-4700-bc1d-c167ff4a2f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00536227,  0.00236431,  0.0510335 ,  0.09009273, -0.0930295 ,\n",
       "       -0.07116809,  0.06458873,  0.08972988, -0.05015428, -0.03763372],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [ [\"i\",\"like\",\"ai\"],[\"ai\",\"is\",\"the\",\"next\"],[\"i\",\"like\",\"machine\",\"learning\"]]\n",
    "\n",
    "#help(Word2Vec)\n",
    "model = Word2Vec(sentences,vector_size=10,window=2,min_count=1,sg=1)\n",
    "model.wv[\"ai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "694ac4f7-51d3-4e44-90ef-514a0ff6e914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 0.5436005592346191),\n",
       " ('i', 0.3004249036312103),\n",
       " ('learning', 0.10494351387023926),\n",
       " ('is', -0.1311161071062088),\n",
       " ('the', -0.1897382140159607),\n",
       " ('machine', -0.22418656945228577),\n",
       " ('next', -0.2726020812988281)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4460266-8486-4890-874a-ed923d8be130",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformer\n",
    "============\n",
    "input embedding - each word into dense vectors\n",
    "positional encoding \n",
    "encoder and decoder \n",
    "attention mechanism \n",
    "Q K V //parameters \n",
    "Query\n",
    "Key\n",
    "Value\n",
    "\n",
    "Input ---> [ Transformer ] --->output\n",
    "               |\n",
    "Input ---> [ Encoder and Decoder ] --->output\n",
    "              ..         ...\n",
    "\n",
    "\n",
    "Input ---> [ FeedForwardNN ] --->output\n",
    "              |\n",
    "             Self-Attention\n",
    "\n",
    "InputTokens ->Embeddings + Positional Encoding\n",
    "                |\n",
    "                Encoder layers (N times)\n",
    "                |\n",
    "                Decoder layers (N times )\n",
    "                |\n",
    "                output probabilities (next word / translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "49acc502-57e7-4624-bd75-76b0fc53d0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: ['i', 'like', 'learning', 'ai']\n",
      "token id: [101, 1045, 2066, 4083, 9932, 102]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # uncase -> box Box both are same\n",
    "text = \"i like learning ai\"                                    # used subword-based method\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"token:\",tokens)\n",
    "print(\"token id:\",token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ed033050-61aa-455a-b523-a1c8650ebcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i like learning ai [SEP]\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(token_ids)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99acb9ff-d77f-4d06-88fc-e325266fe032",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenization ->Transformer \n",
    "\n",
    "Step 1: Tokenization\n",
    "        Token=word,subwords,sentence,character \n",
    "\n",
    "Step 2: text processing\n",
    "        BOW;TF-IDF\n",
    "Step 3: WordEmbedding (DL)\n",
    "         Word2Vec/FastText\n",
    "        word sematic \n",
    "Step 4: Neural Network\n",
    "Step 5: Transformer\n",
    "            |->encoder \n",
    "            |->decoder\n",
    "            |->self-attention - relationship between all the words\n",
    "\"hello\" ->[100s vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e327bb-915e-4fb3-ba8d-c4d24086d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding Model\n",
    "-----------------\n",
    "1. Output(result) -> vector ->[1,2,3..N]\n",
    "2. Each word(token) ->represent numerically\n",
    "3. Usecases: Semantic search,clustering,similarity search\n",
    "4. we won't use any prompting \n",
    "\n",
    "Large Language Model\n",
    "=======================\n",
    "1. Output(result) ->Text\n",
    "2. Usecases: chat,text generation,summarization\n",
    "3. prompt techniques are applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d64ec-72b6-436e-80ce-92c251cdadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "personA  - 100books about list of birds <== Q1: \n",
    "Vs\n",
    "personB  - 50books about list of birds   <== Q1:\n",
    "\n",
    "person - model\n",
    "books - vectors \n",
    "Q1: user prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e5dce-0765-4241-9034-5e4561ca7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############  End of the day1  #####################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
