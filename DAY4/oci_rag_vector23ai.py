import os
import array
import json
import oci
import python_oracledb

from langchain.prompts import PromptTemplate
from langchain.chains import create_stuff_documents_chain, create_retrieval_chain
from langchain.schema import Document
from langchain.embeddings import OllamaEmbeddings  # or your embedding provider

# 1. Configuration / credentials
oracle_config = {
    "user": os.getenv("ORACLE_USER"),
    "password": os.getenv("ORACLE_PW"),
    "dsn": os.getenv("ORACLE_DSN"),  # e.g. "host:1521/service_name"
    # optional: "encoding": "UTF-8" etc.
}
oci_config = oci.config.from_file(  # or other loading method
    file_location=os.getenv("OCI_CONFIG_PATH", "~/.oci/config"),
    profile_name=os.getenv("OCI_PROFILE", "DEFAULT")
)

compartment_id = os.getenv("OCI_COMPARTMENT_OCID")
genai_endpoint = os.getenv("OCI_GENAI_ENDPOINT")  # e.g. "https://inference.generativeai.us-chicago-1.oci.oraclecloud.com"

# 2. Connect to Oracle DB
connection = python_oracledb.connect(**oracle_config)
cursor = connection.cursor()

# 3. Ensure table for vector store exists (example)
cursor.execute("""
CREATE TABLE IF NOT EXISTS docs_vectors (
  doc_id     NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  content    CLOB,
  embedding  VECTOR(1536, FLOAT32)   -- adjust dims/format as your embeddings
)
""")
connection.commit()

# 4. Insert your documents (example) and embeddings
docs = [
    Document(page_content="Python is a general purpose programming language"),
    Document(page_content="Langchain helps build llm apps"),
    Document(page_content="Ollama model its works locally")
]

embedding_obj = OllamaEmbeddings(model="gemma2:2b")  # example

for doc in docs:
    emb = embedding_obj.encode(doc.page_content)  # or get_text_embedding
    # convert to array.array("f", ...) for FLOAT32
    arr = array.array("f", emb)  
    cursor.execute(
        "INSERT INTO docs_vectors(content, embedding) VALUES (:1, :2)",
        [doc.page_content, arr]
    )
connection.commit()

# 5. Build a simple retriever function
def oracle_retrieve(query: str, top_k: int = 3):
    q_emb = embedding_obj.encode(query)
    q_arr = array.array("f", q_emb)
    # query using VECTOR_DISTANCE (assuming FLOAT32 and default distance)
    cursor.execute(f"""
      SELECT content, VECTOR_DISTANCE(embedding, :q) AS dist
      FROM docs_vectors
      ORDER BY dist ASC
      FETCH FIRST :k ROWS ONLY
    """, {"q": q_arr, "k": top_k})
    rows = cursor.fetchall()
    return [Document(page_content=row[0]) for row in rows]

# 6. Setup OCI Generative AI client for LLM
genai_client = oci.generative_ai_inference.GenerativeAiInferenceClient(
    config=oci_config,
    service_endpoint=genai_endpoint
)

def call_llm(prompt: str) -> str:
    details = oci.generative_ai_inference.models.GenerateTextDetails(
        model_id=os.getenv("OCI_MODEL_ID"),   # e.g. "cohere.command-text-v14"
        prompt=prompt,
        max_tokens=200
    )
    resp = genai_client.generate_text(details)
    return resp.data.choices[0].text

# 7. Compose prompt and chain
prompt_template = PromptTemplate.from_template("""
You are a helpful assistant. Use the following documents to answer the questions.
{context}

Question: {input}
Answer:
""")

def run_rag(query: str):
    retrieved = oracle_retrieve(query, top_k=3)
    # build context
    context = "\n\n".join([doc.page_content for doc in retrieved])
    full_prompt = prompt_template.format(context=context, input=query)
    answer = call_llm(full_prompt)
    return answer

# 8. Run
if __name__ == "__main__":
    user_question = "What is Ollama?"
    response = run_rag(user_question)
    print("Answer:", response)
