{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306d9363-7fac-4940-b285-d07d957c5661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0fca4d7-f244-4a35-ab26-d67ef4575851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afdfbcc0-73ed-4112-a38b-09ca8d50a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_obj = ChatGroq(model=\"groq/compound-mini\",api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76954a1-3faa-4bc6-bcde-d5de4eab8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098004a0-91bd-4140-85be-59f4c782f859",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) ->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(llm_obj,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3640aeb3-95e7-49cc-922d-3d66370f9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e6f385-ad52-4969-9016-c9d8eab176d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, Tam! Nice to meet you. How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 445, 'total_tokens': 502, 'completion_time': 0.127528, 'prompt_time': 0.030475, 'queue_time': 0.095322, 'total_time': 0.158003}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--7bf9f4af-174a-4cfc-86c5-b04b77021e96-0', usage_metadata={'input_tokens': 445, 'output_tokens': 57, 'total_tokens': 502})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "with_message_history.invoke(HumanMessage(content=\"Hello my name is Tam\"),config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d797188-cc1a-4ccc-bb08-df135a118f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c682f313-4d2d-4e04-9647-5a0233c41d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bc33688-1c7d-49bd-a169-76652eea1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your are helpful AI assistant,Answer all the question in {language}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"question\")])\n",
    "chain = prompt|llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f3acfb7-b92f-4daf-b266-b060a2af48b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour Leo ! Enchanté de faire votre connaissance. Comment puis-je vous aider aujourd’hui ?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content=\"Hello my name is Leo\")],\"language\":\"French\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b50b3cc2-8105-488e-8dc1-ed6510bd1012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: Your are helpful AI assistant,Answer all the question in English.\\nHuman: Question:English'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted = chat_prompt.format(language=\"English\")\n",
    "formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3b0934f-03e5-40a1-8d86-c65292e92aad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "formatted = chat_prompt.format_messages(\n",
    "    language=\"English\",\n",
    "    question=[\n",
    "        HumanMessage(content=\"What is LangChain?\"),\n",
    "        AIMessage(content=\"LangChain is a framework for building LLM apps.\"),\n",
    "        HumanMessage(content=\"Explain how ChatPromptTemplate works.\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c112f18-50b4-49ea-a949-f72d1c233298",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Your are helpful AI assistant,Answer all the question in English.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Question:English', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2099590d-162b-4a72-b5e7-18d12103790b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bonjour Tom\\u202f! Enchanté de faire votre connaissance.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 480, 'total_tokens': 574, 'completion_time': 0.205417, 'prompt_time': 0.032685, 'queue_time': 0.097011, 'total_time': 0.238103}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4c7fcd68-73d8-48db-99d6-6d94adbd5488-0', usage_metadata={'input_tokens': 480, 'output_tokens': 94, 'total_tokens': 574})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your are helpful AI assistant,Answer all the question in {language}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"question\")])\n",
    "chain = prompt|llm_obj\n",
    "\n",
    "chain.invoke({'question':[HumanMessage(content='Hello,My name is Tom')],\"language\":\"french\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cd550f7-51da-4d1a-a7e8-fda286d77785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour Tom ! Enchanté de faire votre connaissance. Comment puis-je vous aider aujourd’hui ?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content='Hello,My name is Tom')],\"language\":\"french\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e7eb621-1e1b-48c6-ac71-6e9224159e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नमस्ते टॉम! आपसे मिलकर बहुत खुशी हुई। आपका दिन शुभ हो!\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content='Hello,My name is Tom')],\"language\":\"Hindi\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c236603-f366-44e6-8000-aa1ab3d2c6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "హలో టామ్! మీకు స్వాగతం. మీరు ఎలా ఉన్నారు?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content='Hello,My name is Tom')],\"language\":\"telugu\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0a7ac9b-7cd1-4f9b-a6ef-810795d50bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "வணக்கம் டோம்! நீங்கள் எப்படி இருக்கிறீர்கள்?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content='Hello,My name is Tom')],\"language\":\"tamil\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87ee4031-1445-4e25-b84f-9c1671b19b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ನಮಸ್ಕಾರ ಟಾಮ್! ನಿಮಗೆ ಹೇಗಿದೆ? ನಿಮಗೆ ಏನಾದರೂ ಸಹಾಯ ಬೇಕಿದ್ದರೆ ಹೇಳಿ.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content='Hello,My name is Tom')],\"language\":\"kannada\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b17a9f5-e436-4051-846f-b116883ea8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ನಮಸ್ಕಾರ ಟಾಮ್! ನಿಮಗೆ ಹೇಗಿದೆ? ನಿಮಗೆ ಏನಾದರೂ ಸಹಾಯ ಬೇಕಿದ್ದರೆ ಹೇಳಿ.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 306, 'prompt_tokens': 482, 'total_tokens': 788, 'completion_time': 0.733171, 'prompt_time': 0.034764, 'queue_time': 0.102737, 'total_time': 0.767935}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--fb3c044f-6f12-4e1e-b58f-5ac97f39787b-0', usage_metadata={'input_tokens': 482, 'output_tokens': 306, 'total_tokens': 788})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e6d6d-dba5-4368-9c12-4b25a0131ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "Core ideas of prompt engineering\n",
    " |-> LLM - follow the prompt rules and roles\n",
    "\n",
    "shots - an examples - example that we include in the prompt ->to show the model how to respond \n",
    "\n",
    "1. zero-shot prompt -> No examples given\n",
    "Example\n",
    "--------\n",
    "Prompt: \"Translate the following english sentence to french\"\n",
    "         \"I am learning Artifical Intelligence\" \n",
    "Model Output: \"aprends... \"\n",
    "\n",
    "2. one-shot prompt  -> One example given\n",
    "Example\n",
    "--------\n",
    "Prompt: \"Translate the following english sentence to french\"\n",
    "        Example: I likes programming\n",
    "        French: 'aime prgmmer'\n",
    "        Now translate: \n",
    "        English: I am learning Aritfical Intelligence\n",
    "\n",
    "Model Output:  \"aprends... \"\n",
    "\n",
    "3. few-shot prompt -  2 to 5 examples (or) more that\n",
    "------------------\n",
    "Prompt:\n",
    "Classify the sentiment of each sentence as Positive,Negative,Neutral\n",
    "\n",
    "Example 1:\n",
    "sentence: The movie was fantastic \n",
    "sentiment: Positive\n",
    "Example 2:\n",
    "sentence: The food was not good and tasteless\n",
    "sentiment: Negative\n",
    "Example 3:\n",
    "sentence: It was an average day\n",
    "sentiment: Neutral\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "421387cd-71f4-4712-8b06-0595d18073f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Positive', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 493, 'total_tokens': 565, 'completion_time': 0.173934, 'prompt_time': 0.035024, 'queue_time': 0.094861, 'total_time': 0.208958}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3df9b7d7-0ea6-4bab-8ccb-bae00fb11da0-0', usage_metadata={'input_tokens': 493, 'output_tokens': 72, 'total_tokens': 565})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_prompt = ChatPromptTemplate.from_template('''\n",
    "classify the sentiment of the given text as Positive,Negative or Neutral\n",
    "Text:{input}\n",
    "Sentiment:''')\n",
    "\n",
    "zero_shot = zero_shot_prompt.format_messages(input=\"I absolutely loved the movie, it was fantastic!\")\n",
    "llm_obj.invoke(zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e2439e1-d59b-4fc0-950f-434550ba566f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentiment: Positive'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm_obj.invoke(zero_shot)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5340af12-801c-4c31-a4df-a802b2228d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Negative', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 485, 'total_tokens': 606, 'completion_time': 0.316214, 'prompt_time': 0.035663, 'queue_time': 0.097597, 'total_time': 0.351878}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--820c8e06-d9f8-4b66-8923-e60e58e5854e-0', usage_metadata={'input_tokens': 485, 'output_tokens': 121, 'total_tokens': 606})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot = zero_shot_prompt.format_messages(input=\"I not loved the movie!\")\n",
    "llm_obj.invoke(zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28b2c532-d045-409b-bdaa-fde297edf005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Neutral', additional_kwargs={'reasoning_content': '<tool>python(print(\"Neutral\"))</tool>\\n<output>Neutral\\n</output>'}, response_metadata={'token_usage': {'completion_tokens': 127, 'prompt_tokens': 517, 'total_tokens': 644, 'completion_time': 0.342273, 'prompt_time': 0.036729, 'queue_time': 0.092475, 'total_time': 0.379001}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d7adebdd-7c05-40be-8b6b-ab922a5c38d9-0', usage_metadata={'input_tokens': 517, 'output_tokens': 127, 'total_tokens': 644})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot = zero_shot_prompt.format_messages(input=\"The movie was ok\")\n",
    "llm_obj.invoke(zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8fcd7b-68a1-4cd1-bb21-97f64e66bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks:\n",
    "# one-shot\n",
    "# few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ab3dd-2317-4618-b5f1-4e7f6b3c2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task:  Summarization from loaded pdf file \n",
    "1. Loads a pdf file\n",
    "2. Splits into chunks\n",
    "3. Create vector embedding\n",
    "4. Stores them in a Vectorstore\n",
    "\n",
    "\n",
    "5. use RAG to summarize the PDF <===\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "5.1 => Create an llm model \n",
    "5.2 => Build RAG chain ( Retriever + LLM)\n",
    "       qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,reriever=retriever_obj)\n",
    "\n",
    "5.3 => Ask summarization \n",
    "      user_query=\"Summarize the results from this PDF\"\n",
    "      qa_chain.invoke('query':user_query) ->response \n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Task:  Summarization from loaded csv file \n",
    "1. Loads a csv file\n",
    "2. Splits into chunks\n",
    "3. Create vector embedding\n",
    "4. Stores them in a Vectorstore\n",
    "\n",
    "\n",
    "5. use RAG to summarize the PDF <===\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "5.1 => Create an llm model \n",
    "5.2 => Build RAG chain ( Retriever + LLM)\n",
    "       qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,reriever=retriever_obj)\n",
    "\n",
    "5.3 => Ask summarization \n",
    "      user_query=\"Summarize the results from this PDF\"\n",
    "      qa_chain.invoke('query':user_query) ->response \n",
    "---------------------------------------------------Vs\n",
    "fobj = open('input_csv_File.csv')\n",
    "text = fobj.read()\n",
    "fobj.close()\n",
    "....\n",
    "llm_obj.invoke('.....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e13c3c94-2d04-41d2-b4a0-e914a6da21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task:  Summarization from loaded pdf file \n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86b3dc13-efb4-420b-8d52-aea1fc54e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load pdf file\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13cc9305-afd9-404f-b5ce-f74bb1326c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c87b151d-995a-485e-95ba-58245c694b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create embedding \n",
    "embedding_obj = OllamaEmbeddings(model=\"gemma2:2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e78e636f-f2d0-4ac0-a14a-12fde3188eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(docs,embedding_obj)\n",
    "retriever_obj = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24ddc61d-a530-429e-973d-bd15baff79b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create a llm \n",
    "llm_obj = ChatGroq(model=\"groq/compound-mini\",api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddff8fbc-85c0-4b60-a594-624fbc291413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Build RAG chain (Retriever + llm)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7affdc-1703-4e15-aff4-3cbb04de6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Ask Summarization\n",
    "query=\"Summarize the results from this PDF.\"\n",
    "response = qa_chain.invoke({\"query\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef61e55-9ca9-49e3-90e3-99484ed1775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Show results\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd492024-d276-46aa-8a53-7bd4ad56f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grammar,Spelling Check from loaded pdf \n",
    "## Step 7: \n",
    "query='''\n",
    "You are a professional english editor.\n",
    "Correct grammar,punctuation and sentence flow in the following text from the PDF.\n",
    "Improve the readability.\n",
    "Return only the corrected version'''\n",
    "response = qa_chain.invoke({\"query\":query})\n",
    "\n",
    "# Step 8: Show results\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e70ac-4b77-4e2b-b112-11e4d769cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0e939-c213-4c70-af6a-8bdc91555ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search('what is langchain?')\n",
    "//vector search\n",
    "Simple vector search \n",
    "No LLM Processing involved \n",
    "|\n",
    "|\n",
    "vectorstore.similarity_search('what is langchain?')\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "=============\n",
    "The vectorstore in converted to retriever object\n",
    "|\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj)  \n",
    "//Combine document retrieval with QA powered by llm\n",
    "qa_chain('what is langchain?') (or) qa_chain.invoke({\"query\":'what is langchain?'}) ->response\n",
    "used LLM Processing is involved\n",
    "No prompting technique is used\n",
    "|\n",
    "prompting\n",
    "=========\n",
    "prompt | llmobject + chain.invoke({'question': ....}) \n",
    "=======================================================\n",
    "chain.invoke() //prompt based approach \n",
    "|\n",
    "|-> 1. user query -->processed via a prompt template (ex: our chat_prompt)\n",
    "|-> 2. the LLM generates an answer based on the prompt.\n",
    "use case: chatbot\n",
    "LLM's pre-trained knowledge (ex: Input: 1984 PM india ..-->Model1  <== Q: who is PM India?\"\n",
    "Good for generating direct answers to specific questions(query) without needing external\n",
    "loaded documents\n",
    "|\n",
    "\n",
    "prompt+llm_obj+loaded_data\n",
    "\n",
    "create_stuff_documents_chain(llm_obj,prompt) ->chain \n",
    "response = chain.invoke({'input_documents':docs,'questions':'what is langchain?'})\n",
    "Vs\n",
    "create_retrieval_chain(retirever_object,qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f9d93-2a3b-4e25-a809-2b1db531200e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4b6768-cb64-4b30-9a58-84184442f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f58c9d-f199-439f-aa9b-e6ed0f71b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load pdf file\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Create embedding \n",
    "embedding_obj = OllamaEmbeddings(model=\"gemma2:2b\")\n",
    "\n",
    "# Step 4: Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(docs,embedding_obj)\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "# Step 5: Create a llm \n",
    "llm_obj = ChatGroq(model=\"groq/compound-mini\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "# Prompt and chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Use the context to answer:{context}\\n:Question:{input}\")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm_obj,prompt)\n",
    "rag_chain = create_retrieval_chain(retriever_obj,qa_chain)\n",
    "\n",
    "response = rag_chain.invoke({'input':'what is lang-chain?'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4685095-fc96-43a9-94d7-ae58cae25623",
   "metadata": {},
   "outputs": [],
   "source": [
    "User Question ->Retriever -->fetch top matched chunks of text -> document chain -->(A)\n",
    "                                                                 ===============\n",
    "(A) -> {context} and {input} in prompt --> LLM -->FinalAnswer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25253f5f-6765-4f22-9ffc-5a45485216c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efa68bf-e16b-41c7-ac1f-698d30cb6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma # ,FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a132cc20-0a99-4587-bd96-2532ddad1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9f5cf1-2633-4536-930a-5556df0db768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e82ea553-319a-4e8f-9c26-64fb213fc423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is ollama?',\n",
       " 'context': [Document(metadata={}, page_content='Ollama model its works locally'),\n",
       "  Document(metadata={}, page_content='Ollama model its works locally'),\n",
       "  Document(metadata={}, page_content='Langchain helps build llm apps'),\n",
       "  Document(metadata={}, page_content='Langchain helps build llm apps')],\n",
       " 'answer': '**Ollama** is a platform that lets you run large language models locally on your own hardware, enabling you to use LLMs without relying on external cloud services.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load sample data\n",
    "docs = [ Document(page_content=\"Python is a general purpose programming language\"),\n",
    "        Document(page_content=\"Langchain helps build llm apps\"),\n",
    "        Document(page_content=\"Ollama model its works locally\")]\n",
    "\n",
    "# create the prompt\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Use the following documents to answer the questions.\n",
    "{context}\n",
    "Question:{input}\n",
    "Answer:\"\"\")\n",
    "\n",
    "embedding_obj = OllamaEmbeddings(model=\"gemma2:2b\")\n",
    "\n",
    "vectordb = Chroma.from_documents(docs,embedding=embedding_obj)\n",
    "retriever_obj = vectordb.as_retriever()\n",
    "\n",
    "# llm \n",
    "llm_obj = ChatGroq(model=\"groq/compound-mini\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "# create the chain\n",
    "stuff_chain = create_stuff_documents_chain(llm=llm_obj,prompt=prompt)\n",
    "rag_chain =create_retrieval_chain(retriever=retriever_obj,combine_docs_chain=stuff_chain)\n",
    "\n",
    "# run\n",
    "rag_chain.invoke({\"input\":\"what is ollama?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ea970d4-dc7c-4418-ab1f-25abecd4d409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Ollama** is a platform that lets you run large language models locally on your own hardware, enabling offline inference without relying on external APIs.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\":\"what is ollama?\"})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bd2933d-bbab-4c46-baa2-a7422760946c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Use the following documents to answer the questions.\\n{context}\\nQuestion:{input}\\nAnswer:')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1896a1a0-e021-4b52-a041-3a8b3110f001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
