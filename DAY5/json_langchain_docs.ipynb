{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c33602-d46e-47ed-bf4f-8391f1c70bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_json_to_documents(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    docs = []\n",
    "    for entry in data:\n",
    "        content = entry.get(\"content\", \"\")\n",
    "        metadata = {k: v for k, v in entry.items() if k != \"content\"}\n",
    "        docs.append(Document(page_content=content, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "docs = load_json_to_documents(\"data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e2ae2d0-5cba-4306-acda-b2ea9ca306c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_22088\\1138914918.py:10: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding = OllamaEmbeddings(model=\"gemma:2b\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Split into smaller chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# Use Ollama for embeddings (make sure Ollama is running)\n",
    "embedding = OllamaEmbeddings(model=\"gemma:2b\")\n",
    "\n",
    "# Create vector DB\n",
    "vectordb = Chroma.from_documents(split_docs, embedding=embedding)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d44695b-80b8-4023-a38a-01406e68176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "#from langchain.chains import create_stuff_documents_chain, create_retrieval_chain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Define your prompt\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert assistant. Use the below context to answer the question.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "Answer:\"\"\")\n",
    "\n",
    "# Connect LLM\n",
    "llm = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "# Create \"stuff\" chain to combine retrieved docs\n",
    "doc_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Full RAG chain\n",
    "rag_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=doc_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1142db34-1667-4666-808a-487de42cee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'input': 'What is Ollama?', 'context': [Document(metadata={'title': 'What is Ollama?'}, page_content='Ollama allows running large language models locally like LLaMA and Mistral.'), Document(metadata={'title': 'LangChain Overview'}, page_content='LangChain is a framework for developing applications powered by language models.'), Document(metadata={'title': 'Embeddings in NLP'}, page_content='Embeddings are vector representations of text used for similarity and retrieval.')], 'answer': 'The context does not provide information about what Ollama is, so I cannot answer this question from the provided context.'}\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Ollama?\"\n",
    "response = rag_chain.invoke({\"input\":question})\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166670b4-40e1-4b61-8f0d-931ccb543ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
