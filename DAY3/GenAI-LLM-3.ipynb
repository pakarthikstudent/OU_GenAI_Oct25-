{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4465a-70ad-4d7f-8565-42a1307b1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recap\n",
    "=======\n",
    "DAY1 - Tokenization stem lemma - embedding - text ->vector\n",
    "                                             image ->pixel ->vector\n",
    "                                            audio/video ... ->vector\n",
    "                                        ============================\n",
    "DAY2 - langchain - framework - llm-model ->genAI ->LLMApplications\n",
    "        |->langchain_community \n",
    "            |->document loaders ; splitter ; vectorstore \n",
    "DataLoading \n",
    " - TextLoader\n",
    " - PyPDFLoader\n",
    " - WebbaseLoader\n",
    " -             |->web_path - bs_kwargs \n",
    " - WikipediaLoader \n",
    " - ..\n",
    "Split - data into multiple chunks => [][][][][] \n",
    "|\n",
    "Embedding - Embedded_object \n",
    "|\n",
    "Created Vector store - Stores to vector DB\n",
    "|\n",
    "similarity search\n",
    "=====================================================================\n",
    "Ollama model ->downloaded ->There is no env updated \n",
    "------------//act as a service - running on local instance \n",
    "HuggingFace -> created userAccount ->AccessToken =>updated this token .env file \n",
    "                env variable name HF_TOKEN=\"access_token\"\n",
    "|\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ->True \n",
    "os.environ['HF_TOKEN'] = os.getenviron('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4919f-ddbc-4876-bd6a-f6322d6fee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Groq -\n",
    " - LPU <== high throughput,efficient performance ; low latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8678f49e-c972-43bb-80de-b1efeec14e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287933ba-73a9-4c64-b24b-3b07e9e986eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karth'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d67732-180a-4393-ad50-bcddb608a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain-core langchain langchain-community <== document loaders,embedding,vectorstore\n",
    "! pip install langchain-groq <== to use groq model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102e4517-200b-4a47-9452-d5263566caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c2d7ad-b633-438e-987e-bc355937cb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_groq.chat_models.ChatGroq'>\n"
     ]
    }
   ],
   "source": [
    "print(ChatGroq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dfefc0-6d32-499e-9184-338e89518e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70dbfe4-b8b1-4b7e-81e0-fd736cff8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import <module>\n",
    "<module>.<member>\n",
    "\n",
    "from <module> import <member>\n",
    "<member>\n",
    "\n",
    "import dir1.dir2.module \n",
    "dir1.dir2.module.member\n",
    "\n",
    "import dir1.dir2.module as mymodule\n",
    "mymodule.member\n",
    "\n",
    "import html_template_filter\n",
    "html_template_filter.header()\n",
    "\n",
    "import html_template_filter as myhf\n",
    "myhf.header()\n",
    "\n",
    "from dir1.dir2.module import member\n",
    "member\n",
    "\n",
    ">>> import os\n",
    ">>>\n",
    ">>> os.system(\"whoami\")\n",
    "paka\\karth\n",
    "0\n",
    ">>> system(\"whoami\")\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "NameError: name 'system' is not defined\n",
    ">>>\n",
    ">>> from os import system\n",
    ">>> system(\"whoami\")\n",
    "paka\\karth\n",
    "0\n",
    ">>>\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2d85bd-b5e6-4ef9-bc52-ffac5d36c795",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `llama-3-groq-8b-tool-use` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_groq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[0;32m      2\u001b[0m llm_obj \u001b[38;5;241m=\u001b[39m ChatGroq(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama-3-Groq-8B-Tool-Use\u001b[39m\u001b[38;5;124m\"\u001b[39m,api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m llm_obj\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is langchain?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    396\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    397\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    398\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    399\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    400\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    401\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    402\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    403\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    404\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1025\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1023\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m   1024\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:842\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    841\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 842\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    843\u001b[0m                 m,\n\u001b[0;32m    844\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    845\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    846\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    847\u001b[0m             )\n\u001b[0;32m    848\u001b[0m         )\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    850\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1091\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1091\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1092\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1093\u001b[0m     )\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1095\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:557\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    553\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    556\u001b[0m }\n\u001b[1;32m--> 557\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, params)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:368\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    370\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    371\u001b[0m             {\n\u001b[0;32m    372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    373\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    374\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: exclude_domains,\n\u001b[0;32m    375\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_domains,\n\u001b[0;32m    379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    386\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    387\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[0;32m    389\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    390\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m: search_settings,\n\u001b[0;32m    391\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    392\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    393\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    394\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    395\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    396\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    397\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    398\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    399\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    400\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    401\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    402\u001b[0m             },\n\u001b[0;32m    403\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    404\u001b[0m         ),\n\u001b[0;32m    405\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    406\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    407\u001b[0m         ),\n\u001b[0;32m    408\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    409\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    410\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    411\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1232\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1220\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1228\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1229\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1230\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1231\u001b[0m     )\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `llama-3-groq-8b-tool-use` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm_obj = ChatGroq(model=\"Llama-3-Groq-8B-Tool-Use\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "llm_obj.invoke(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf73ec-bd3c-4c1c-8a6f-ad369584f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0045fe-c986-47c6-873c-61b222ff3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_obj.invoke(\"what is langchain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483d4e7-6105-4e53-9a2e-874e81ef961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gemma‑7B\n",
    "Llama2‑70B‑4096\n",
    "Llama3‑8B‑8192\n",
    "Llama‑3‑Groq‑8B‑Tool‑Use\n",
    "Llama‑3‑Groq‑70B‑Tool‑Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71268310-d23c-445f-95a0-45f9b55913bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Delhi is the capital city of India. It is located in the northern part of the country, on the banks of the Yamuna River. Geographically, its approximate coordinates are **28.61°\\u202fN latitude and 77.23°\\u202fE longitude**. It lies about 200\\u202fkm (≈125\\u202fmi) south of the Himalayan foothills and is bordered by the state of Haryana on three sides and by Uttar\\u202fPradesh to the east.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 202, 'prompt_tokens': 443, 'total_tokens': 645, 'completion_time': 0.466862, 'prompt_time': 0.031915, 'queue_time': 0.300912, 'total_time': 0.498776}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e152d056-96e9-44a7-a9b7-f0f7d3a075a0-0', usage_metadata={'input_tokens': 443, 'output_tokens': 202, 'total_tokens': 645})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm_obj = ChatGroq(model=\"groq/compound-mini\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "llm_obj.invoke(\"where is Delhi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd8db1c-3b5b-4e5d-bfaa-a75ab3d54981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Artificial Intelligence (AI) is a branch of computer science focused on creating machines and software that can perform tasks typically requiring human intelligence. These tasks include learning from data, recognizing patterns, understanding natural language, solving problems, and making decisions. AI systems range from simple rule‑based programs to advanced models like neural networks that can learn and improve over time. In short, AI aims to give computers the ability to think, learn, and act in ways that resemble human cognition.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 303, 'prompt_tokens': 443, 'total_tokens': 746, 'completion_time': 0.74323, 'prompt_time': 0.031805, 'queue_time': 0.100071, 'total_time': 0.775035}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9ac8b7ac-f594-401e-8b88-5a350499f271-0', usage_metadata={'input_tokens': 443, 'output_tokens': 303, 'total_tokens': 746})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_obj.invoke(\"what is AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de2a908-afd1-48d4-a18a-40865d61da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "TextLoader().load() ->splitter.split_documents()->embedding->vector_store\n",
    "\n",
    "vector_store.similarity_search('Query') -> ...\n",
    "\n",
    "Retriever = fetchs the most relevent documents from database(vectorstore) base on query\n",
    "|\n",
    "LLM - llm reads the retrieved documents then Generates Answer\n",
    "lanchain.chains import RetrievalQA\n",
    "\n",
    "RetrievalQA.from_chain_type(llm=<>,retriever=<>) ->QAChain\n",
    "\n",
    "vectorstore.as_reriever()\n",
    "\n",
    "==============================\n",
    "1. vector store\n",
    "vector_store = FAISS.from_documents(docs,embedding=embedded_obj)\n",
    "\n",
    "Aritfical intelligence is transforming inds ->[0.12,-0.33,0.91...]\n",
    "AI used .... ->[0.13,-0.32,0.92...]\n",
    "\n",
    "2. Q: what is AI?\n",
    "\n",
    "retriever = like search engine - collected the relevent data ==> sends them to LLM\n",
    "vector_store.as_retriever() ->reriver_obj\n",
    "reriver_obj.get_relevant_documents(''.......k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55eeece-2d8f-4d7b-bba1-5a5a3e64ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88d2e88-8e76-4f86-876a-2eda14f45907",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_53412\\1783078021.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# step-1\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# step-2\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# step-3\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# step-4\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "# step-5\n",
    "# vectorstore.similarity_search(\"what is langchain?\")\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "# step-6 - llm object\n",
    "llm_obj = ChatGroq(model=\"groq/compound-mini\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "# step-7 - Build Retrieval QA Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311d65d3-553f-4966-b916-f56af0df96c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_53412\\3661671904.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain(\"what is langchain?\")\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is langchain?', 'result': 'LangChain is an open‑source framework for building applications that use large language models (LLMs). It provides reusable components and integrations to simplify every step of the LLM app lifecycle—development (including stateful agents with LangGraph), production monitoring and evaluation with LangSmith, and deployment of production‑ready APIs and assistants via the LangGraph Platform.'}\n"
     ]
    }
   ],
   "source": [
    "# step-8 \n",
    "response = qa_chain(\"what is langchain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa29e7b-01fa-41ac-99b9-6906c9c5047e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    ">>> class box:\n",
    "...     def __init__(self):\n",
    "...             print(\"Welcome\")\n",
    "...\n",
    ">>> obj = box()\n",
    "Welcome\n",
    ">>> obj()\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "TypeError: 'box' object is not callable\n",
    ">>>\n",
    ">>> callable(obj)\n",
    "False\n",
    ">>>\n",
    ">>> def fx():\n",
    "...     print(\"Hello\")\n",
    "...\n",
    ">>> type(fx)\n",
    "<class 'function'>\n",
    ">>>\n",
    ">>> callable(fx)\n",
    "True\n",
    ">>> fx.__call__()\n",
    "Hello\n",
    ">>>\n",
    ">>> class box:\n",
    "...     def __init__(self):\n",
    "...             pass\n",
    "...     def __call__(self):\n",
    "...             return \"Hello\"\n",
    "...\n",
    ">>> obj = box()\n",
    ">>> obj()\n",
    "'Hello'\n",
    ">>> callable(obj)\n",
    "True\n",
    ">>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5d4e61d-a035-479c-9a38-4df8f3bf2413",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangChain**\n",
      "\n",
      "LangChain is an open‑source framework that helps you build applications powered by large language models (LLMs).  \n",
      "It provides ready‑made building blocks for every step of the LLM‑app lifecycle:\n",
      "\n",
      "| Stage | What LangChain gives you |\n",
      "|-------|--------------------------|\n",
      "| **Development** | Re‑usable components (prompts, chains, memory, agents, etc.) and integrations with vector stores, databases, APIs, etc. You can also use **LangGraph** to create stateful, multi‑step agents that support streaming and human‑in‑the‑loop interaction. |\n",
      "| **Productionization** | **LangSmith** lets you inspect, monitor, log, and evaluate runs of your chains/agents, making it easy to spot errors, measure performance, and continuously improve the system. |\n",
      "| **Deployment** | With the **LangGraph Platform** you can turn your LangGraph‑based agents into production‑ready APIs or assistants that can be called from any client. |\n",
      "\n",
      "In short, LangChain abstracts away the plumbing (prompt handling, context management, tool use, tracing, etc.) so you can focus on the logic of your application.\n",
      "\n",
      "---\n",
      "\n",
      "**Retrieval‑Augmented Generation (RAG)**\n",
      "\n",
      "RAG is a pattern for combining an LLM’s generative abilities with external knowledge sources, typically a vector‑store or search index. The workflow looks like this:\n",
      "\n",
      "1. **User query** → The LLM receives the question.\n",
      "2. **Retriever** → A separate component (e.g., a dense vector search over embeddings, a traditional BM25 search, or a hybrid) fetches the most relevant documents/chunks from a knowledge base.\n",
      "3. **Context construction** → The retrieved pieces are concatenated (often with a prompt template) and supplied to the LLM as additional context.\n",
      "4. **Generation** → The LLM generates an answer, now “augmented” by the retrieved information, which helps it stay factual and up‑to‑date.\n",
      "5. **(Optional) Reranking / Filtering** → Some pipelines rerank the retrieved chunks or filter out low‑quality ones before feeding them to the model.\n",
      "\n",
      "Why use RAG?\n",
      "\n",
      "- **Grounded answers** – The model can cite specific passages, reducing hallucinations.\n",
      "- **Scalability** – You can keep a huge corpus (millions of docs) in a vector store without having to fine‑tune the LLM.\n",
      "- **Up‑datability** – Adding or updating documents in the store instantly changes the knowledge the system can use.\n",
      "\n",
      "**How LangChain helps with RAG**\n",
      "\n",
      "LangChain already has first‑class components for the whole RAG pipeline:\n",
      "\n",
      "| Component | Role |\n",
      "|-----------|------|\n",
      "| **Document loaders** | Pull data from PDFs, webpages, databases, etc. |\n",
      "| **Text splitters** | Break large texts into manageable chunks (e.g., by token count). |\n",
      "| **Embeddings models** | Convert chunks into vectors (OpenAI, Cohere, HuggingFace, etc.). |\n",
      "| **Vector stores** | Store and search embeddings (FAISS, Pinecone, Chroma, Weaviate, etc.). |\n",
      "| **Retrievers** | Abstract the search logic (similarity search, hybrid search, etc.). |\n",
      "| **Prompt templates** | Build the “question + retrieved context” prompt that is sent to the LLM. |\n",
      "| **Chains / Agents** | Wire all the steps together, optionally adding post‑processing or tool calls. |\n",
      "| **LangSmith** | Monitor latency, token usage, and answer quality for each RAG call. |\n",
      "\n",
      "Putting it together, a typical LangChain RAG chain might look like:\n",
      "\n",
      "```python\n",
      "from langchain import PromptTemplate, LLMChain\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.retrievers import VectorStoreRetriever\n",
      "\n",
      "# 1. Load & index docs (once)\n",
      "docs = loader.load()\n",
      "chunks = splitter.split_documents(docs)\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
      "\n",
      "# 2. Build retriever\n",
      "retriever = VectorStoreRetriever(vectorstore=vectorstore, k=4)\n",
      "\n",
      "# 3. Prompt template\n",
      "prompt = PromptTemplate(\n",
      "    template=\"Answer the question using only the following context:\\n{context}\\n\\nQuestion: {question}\",\n",
      "    input_variables=[\"context\", \"question\"]\n",
      ")\n",
      "\n",
      "# 4. Chain that does retrieval + generation\n",
      "rag_chain = (\n",
      "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
      "    | prompt\n",
      "    | llm\n",
      ")\n",
      "\n",
      "answer = rag_chain.invoke(\"What are the health benefits of turmeric?\")\n",
      "```\n",
      "\n",
      "The chain automatically:\n",
      "\n",
      "1. Retrieves the top‑k relevant chunks (`retriever`).  \n",
      "2. Formats them (`format_docs`).  \n",
      "3. Inserts them into the prompt (`prompt`).  \n",
      "4. Calls the LLM to generate the final answer.\n",
      "\n",
      "---\n",
      "\n",
      "**Summary**\n",
      "\n",
      "- **LangChain** = a modular toolkit for building, monitoring, and deploying LLM‑powered apps.  \n",
      "- **RAG** = a technique that fetches relevant external documents and feeds them to the LLM so it can generate answers grounded in real data.  \n",
      "- LangChain provides all the plumbing (loaders, splitters, embeddings, vector stores, retrievers, prompt templates, chains, and observability) needed to implement a robust RAG pipeline quickly.\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain(\"what is langchain and how RAG is working?\")\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "277f99ea-b044-4590-8853-e887305489b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m sorry, but I don’t understand your request. Could you please clarify what you’d like to know?\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain(\"asfdsa423423adasdasr4@$@EAcASF?\")\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82fa96-c47d-44c5-a76d-46b529ac3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "On mac os\n",
    "=============\n",
    "Use python 3.12\n",
    "ModuleNotFoundError: No module named 'langchain_core.memory'\n",
    "pip uninstall -y langchain langchain-core langchain-community langchainhub\n",
    "pip install -U pip setuptools wheel\n",
    "pip install \"langchain==0.3.7\" \"langchain-core==0.3.17\" \"langchain-community==0.3.7\"\n",
    "\n",
    "|\n",
    "restart kernel \n",
    "|\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"gemma2:2b\")\n",
    "memory = ConversationBufferMemory()\n",
    "chain = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "print(chain.run(\"Hello, I'm using LangChain on macOS!\"))\n",
    "print(chain.run(\"What did I just tell you?\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
