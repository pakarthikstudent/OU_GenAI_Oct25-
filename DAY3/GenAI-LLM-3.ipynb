{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4465a-70ad-4d7f-8565-42a1307b1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recap\n",
    "=======\n",
    "DAY1 - Tokenization stem lemma - embedding - text ->vector\n",
    "                                             image ->pixel ->vector\n",
    "                                            audio/video ... ->vector\n",
    "                                        ============================\n",
    "DAY2 - langchain - framework - llm-model ->genAI ->LLMApplications\n",
    "        |->langchain_community \n",
    "            |->document loaders ; splitter ; vectorstore \n",
    "DataLoading \n",
    " - TextLoader\n",
    " - PyPDFLoader\n",
    " - WebbaseLoader\n",
    " -             |->web_path - bs_kwargs \n",
    " - WikipediaLoader \n",
    " - ..\n",
    "Split - data into multiple chunks => [][][][][] \n",
    "|\n",
    "Embedding - Embedded_object \n",
    "|\n",
    "Created Vector store - Stores to vector DB\n",
    "|\n",
    "similarity search\n",
    "=====================================================================\n",
    "Ollama model ->downloaded ->There is no env updated \n",
    "------------//act as a service - running on local instance \n",
    "HuggingFace -> created userAccount ->AccessToken =>updated this token .env file \n",
    "                env variable name HF_TOKEN=\"access_token\"\n",
    "|\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ->True \n",
    "os.environ['HF_TOKEN'] = os.getenviron('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4919f-ddbc-4876-bd6a-f6322d6fee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Groq -\n",
    " - LPU <== high throughput,efficient performance ; low latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8678f49e-c972-43bb-80de-b1efeec14e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287933ba-73a9-4c64-b24b-3b07e9e986eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karth'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d67732-180a-4393-ad50-bcddb608a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain-core langchain langchain-community <== document loaders,embedding,vectorstore\n",
    "! pip install langchain-groq <== to use groq model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102e4517-200b-4a47-9452-d5263566caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c2d7ad-b633-438e-987e-bc355937cb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_groq.chat_models.ChatGroq'>\n"
     ]
    }
   ],
   "source": [
    "print(ChatGroq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dfefc0-6d32-499e-9184-338e89518e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70dbfe4-b8b1-4b7e-81e0-fd736cff8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import <module>\n",
    "<module>.<member>\n",
    "\n",
    "from <module> import <member>\n",
    "<member>\n",
    "\n",
    "import dir1.dir2.module \n",
    "dir1.dir2.module.member\n",
    "\n",
    "import dir1.dir2.module as mymodule\n",
    "mymodule.member\n",
    "\n",
    "import html_template_filter\n",
    "html_template_filter.header()\n",
    "\n",
    "import html_template_filter as myhf\n",
    "myhf.header()\n",
    "\n",
    "from dir1.dir2.module import member\n",
    "member\n",
    "\n",
    ">>> import os\n",
    ">>>\n",
    ">>> os.system(\"whoami\")\n",
    "paka\\karth\n",
    "0\n",
    ">>> system(\"whoami\")\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "NameError: name 'system' is not defined\n",
    ">>>\n",
    ">>> from os import system\n",
    ">>> system(\"whoami\")\n",
    "paka\\karth\n",
    "0\n",
    ">>>\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2d85bd-b5e6-4ef9-bc52-ffac5d36c795",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `llama-3-groq-8b-tool-use` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_groq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[0;32m      2\u001b[0m llm_obj \u001b[38;5;241m=\u001b[39m ChatGroq(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama-3-Groq-8B-Tool-Use\u001b[39m\u001b[38;5;124m\"\u001b[39m,api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m llm_obj\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is langchain?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    396\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    397\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    398\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    399\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    400\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    401\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    402\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    403\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    404\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1025\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1023\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m   1024\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:842\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    841\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 842\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    843\u001b[0m                 m,\n\u001b[0;32m    844\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    845\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    846\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    847\u001b[0m             )\n\u001b[0;32m    848\u001b[0m         )\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    850\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1091\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1091\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1092\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1093\u001b[0m     )\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1095\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:557\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    553\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    556\u001b[0m }\n\u001b[1;32m--> 557\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, params)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:368\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    370\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    371\u001b[0m             {\n\u001b[0;32m    372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    373\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    374\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: exclude_domains,\n\u001b[0;32m    375\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_domains,\n\u001b[0;32m    379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    386\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    387\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[0;32m    389\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    390\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m: search_settings,\n\u001b[0;32m    391\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    392\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    393\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    394\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    395\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    396\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    397\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    398\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    399\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    400\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    401\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    402\u001b[0m             },\n\u001b[0;32m    403\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    404\u001b[0m         ),\n\u001b[0;32m    405\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    406\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    407\u001b[0m         ),\n\u001b[0;32m    408\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    409\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    410\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    411\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1232\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1220\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1228\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1229\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1230\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1231\u001b[0m     )\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `llama-3-groq-8b-tool-use` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm_obj = ChatGroq(model=\"Llama-3-Groq-8B-Tool-Use\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "llm_obj.invoke(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf73ec-bd3c-4c1c-8a6f-ad369584f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0045fe-c986-47c6-873c-61b222ff3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_obj.invoke(\"what is langchain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483d4e7-6105-4e53-9a2e-874e81ef961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gemma‑7B\n",
    "Llama2‑70B‑4096\n",
    "Llama3‑8B‑8192\n",
    "Llama‑3‑Groq‑8B‑Tool‑Use\n",
    "Llama‑3‑Groq‑70B‑Tool‑Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71268310-d23c-445f-95a0-45f9b55913bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Delhi is the capital city of India. It is located in the northern part of the country, on the banks of the Yamuna River. Geographically, its approximate coordinates are **28.61°\\u202fN latitude and 77.23°\\u202fE longitude**. It lies about 200\\u202fkm (≈125\\u202fmi) south of the Himalayan foothills and is bordered by the state of Haryana on three sides and by Uttar\\u202fPradesh to the east.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 202, 'prompt_tokens': 443, 'total_tokens': 645, 'completion_time': 0.466862, 'prompt_time': 0.031915, 'queue_time': 0.300912, 'total_time': 0.498776}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e152d056-96e9-44a7-a9b7-f0f7d3a075a0-0', usage_metadata={'input_tokens': 443, 'output_tokens': 202, 'total_tokens': 645})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm_obj = ChatGroq(model=\"groq/compound-mini\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "llm_obj.invoke(\"where is Delhi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd8db1c-3b5b-4e5d-bfaa-a75ab3d54981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Artificial Intelligence (AI) is a branch of computer science focused on creating machines and software that can perform tasks typically requiring human intelligence. These tasks include learning from data, recognizing patterns, understanding natural language, solving problems, and making decisions. AI systems range from simple rule‑based programs to advanced models like neural networks that can learn and improve over time. In short, AI aims to give computers the ability to think, learn, and act in ways that resemble human cognition.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 303, 'prompt_tokens': 443, 'total_tokens': 746, 'completion_time': 0.74323, 'prompt_time': 0.031805, 'queue_time': 0.100071, 'total_time': 0.775035}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9ac8b7ac-f594-401e-8b88-5a350499f271-0', usage_metadata={'input_tokens': 443, 'output_tokens': 303, 'total_tokens': 746})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_obj.invoke(\"what is AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de2a908-afd1-48d4-a18a-40865d61da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "TextLoader().load() ->splitter.split_documents()->embedding->vector_store\n",
    "\n",
    "vector_store.similarity_search('Query') -> ...\n",
    "\n",
    "Retriever = fetchs the most relevent documents from database(vectorstore) base on query\n",
    "|\n",
    "LLM - llm reads the retrieved documents then Generates Answer\n",
    "lanchain.chains import RetrievalQA\n",
    "\n",
    "RetrievalQA.from_chain_type(llm=<>,retriever=<>) ->QAChain\n",
    "\n",
    "vectorstore.as_reriever()\n",
    "\n",
    "==============================\n",
    "1. vector store\n",
    "vector_store = FAISS.from_documents(docs,embedding=embedded_obj)\n",
    "\n",
    "Aritfical intelligence is transforming inds ->[0.12,-0.33,0.91...]\n",
    "AI used .... ->[0.13,-0.32,0.92...]\n",
    "\n",
    "2. Q: what is AI?\n",
    "\n",
    "retriever = like search engine - collected the relevent data ==> sends them to LLM\n",
    "vector_store.as_retriever() ->reriver_obj\n",
    "reriver_obj.get_relevant_documents(''.......k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55eeece-2d8f-4d7b-bba1-5a5a3e64ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88d2e88-8e76-4f86-876a-2eda14f45907",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_53412\\1783078021.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# step-1\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# step-2\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# step-3\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# step-4\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "# step-5\n",
    "# vectorstore.similarity_search(\"what is langchain?\")\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "# step-6 - llm object\n",
    "llm_obj = ChatGroq(model=\"groq/compound-mini\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "# step-7 - Build Retrieval QA Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311d65d3-553f-4966-b916-f56af0df96c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_53412\\3661671904.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain(\"what is langchain?\")\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is langchain?', 'result': 'LangChain is an open‑source framework for building applications that use large language models (LLMs). It provides reusable components and integrations to simplify every step of the LLM app lifecycle—development (including stateful agents with LangGraph), production monitoring and evaluation with LangSmith, and deployment of production‑ready APIs and assistants via the LangGraph Platform.'}\n"
     ]
    }
   ],
   "source": [
    "# step-8 \n",
    "response = qa_chain(\"what is langchain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa29e7b-01fa-41ac-99b9-6906c9c5047e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    ">>> class box:\n",
    "...     def __init__(self):\n",
    "...             print(\"Welcome\")\n",
    "...\n",
    ">>> obj = box()\n",
    "Welcome\n",
    ">>> obj()\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "TypeError: 'box' object is not callable\n",
    ">>>\n",
    ">>> callable(obj)\n",
    "False\n",
    ">>>\n",
    ">>> def fx():\n",
    "...     print(\"Hello\")\n",
    "...\n",
    ">>> type(fx)\n",
    "<class 'function'>\n",
    ">>>\n",
    ">>> callable(fx)\n",
    "True\n",
    ">>> fx.__call__()\n",
    "Hello\n",
    ">>>\n",
    ">>> class box:\n",
    "...     def __init__(self):\n",
    "...             pass\n",
    "...     def __call__(self):\n",
    "...             return \"Hello\"\n",
    "...\n",
    ">>> obj = box()\n",
    ">>> obj()\n",
    "'Hello'\n",
    ">>> callable(obj)\n",
    "True\n",
    ">>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5d4e61d-a035-479c-9a38-4df8f3bf2413",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangChain**\n",
      "\n",
      "LangChain is an open‑source framework that helps you build applications powered by large language models (LLMs).  \n",
      "It provides ready‑made building blocks for every step of the LLM‑app lifecycle:\n",
      "\n",
      "| Stage | What LangChain gives you |\n",
      "|-------|--------------------------|\n",
      "| **Development** | Re‑usable components (prompts, chains, memory, agents, etc.) and integrations with vector stores, databases, APIs, etc. You can also use **LangGraph** to create stateful, multi‑step agents that support streaming and human‑in‑the‑loop interaction. |\n",
      "| **Productionization** | **LangSmith** lets you inspect, monitor, log, and evaluate runs of your chains/agents, making it easy to spot errors, measure performance, and continuously improve the system. |\n",
      "| **Deployment** | With the **LangGraph Platform** you can turn your LangGraph‑based agents into production‑ready APIs or assistants that can be called from any client. |\n",
      "\n",
      "In short, LangChain abstracts away the plumbing (prompt handling, context management, tool use, tracing, etc.) so you can focus on the logic of your application.\n",
      "\n",
      "---\n",
      "\n",
      "**Retrieval‑Augmented Generation (RAG)**\n",
      "\n",
      "RAG is a pattern for combining an LLM’s generative abilities with external knowledge sources, typically a vector‑store or search index. The workflow looks like this:\n",
      "\n",
      "1. **User query** → The LLM receives the question.\n",
      "2. **Retriever** → A separate component (e.g., a dense vector search over embeddings, a traditional BM25 search, or a hybrid) fetches the most relevant documents/chunks from a knowledge base.\n",
      "3. **Context construction** → The retrieved pieces are concatenated (often with a prompt template) and supplied to the LLM as additional context.\n",
      "4. **Generation** → The LLM generates an answer, now “augmented” by the retrieved information, which helps it stay factual and up‑to‑date.\n",
      "5. **(Optional) Reranking / Filtering** → Some pipelines rerank the retrieved chunks or filter out low‑quality ones before feeding them to the model.\n",
      "\n",
      "Why use RAG?\n",
      "\n",
      "- **Grounded answers** – The model can cite specific passages, reducing hallucinations.\n",
      "- **Scalability** – You can keep a huge corpus (millions of docs) in a vector store without having to fine‑tune the LLM.\n",
      "- **Up‑datability** – Adding or updating documents in the store instantly changes the knowledge the system can use.\n",
      "\n",
      "**How LangChain helps with RAG**\n",
      "\n",
      "LangChain already has first‑class components for the whole RAG pipeline:\n",
      "\n",
      "| Component | Role |\n",
      "|-----------|------|\n",
      "| **Document loaders** | Pull data from PDFs, webpages, databases, etc. |\n",
      "| **Text splitters** | Break large texts into manageable chunks (e.g., by token count). |\n",
      "| **Embeddings models** | Convert chunks into vectors (OpenAI, Cohere, HuggingFace, etc.). |\n",
      "| **Vector stores** | Store and search embeddings (FAISS, Pinecone, Chroma, Weaviate, etc.). |\n",
      "| **Retrievers** | Abstract the search logic (similarity search, hybrid search, etc.). |\n",
      "| **Prompt templates** | Build the “question + retrieved context” prompt that is sent to the LLM. |\n",
      "| **Chains / Agents** | Wire all the steps together, optionally adding post‑processing or tool calls. |\n",
      "| **LangSmith** | Monitor latency, token usage, and answer quality for each RAG call. |\n",
      "\n",
      "Putting it together, a typical LangChain RAG chain might look like:\n",
      "\n",
      "```python\n",
      "from langchain import PromptTemplate, LLMChain\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.retrievers import VectorStoreRetriever\n",
      "\n",
      "# 1. Load & index docs (once)\n",
      "docs = loader.load()\n",
      "chunks = splitter.split_documents(docs)\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
      "\n",
      "# 2. Build retriever\n",
      "retriever = VectorStoreRetriever(vectorstore=vectorstore, k=4)\n",
      "\n",
      "# 3. Prompt template\n",
      "prompt = PromptTemplate(\n",
      "    template=\"Answer the question using only the following context:\\n{context}\\n\\nQuestion: {question}\",\n",
      "    input_variables=[\"context\", \"question\"]\n",
      ")\n",
      "\n",
      "# 4. Chain that does retrieval + generation\n",
      "rag_chain = (\n",
      "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
      "    | prompt\n",
      "    | llm\n",
      ")\n",
      "\n",
      "answer = rag_chain.invoke(\"What are the health benefits of turmeric?\")\n",
      "```\n",
      "\n",
      "The chain automatically:\n",
      "\n",
      "1. Retrieves the top‑k relevant chunks (`retriever`).  \n",
      "2. Formats them (`format_docs`).  \n",
      "3. Inserts them into the prompt (`prompt`).  \n",
      "4. Calls the LLM to generate the final answer.\n",
      "\n",
      "---\n",
      "\n",
      "**Summary**\n",
      "\n",
      "- **LangChain** = a modular toolkit for building, monitoring, and deploying LLM‑powered apps.  \n",
      "- **RAG** = a technique that fetches relevant external documents and feeds them to the LLM so it can generate answers grounded in real data.  \n",
      "- LangChain provides all the plumbing (loaders, splitters, embeddings, vector stores, retrievers, prompt templates, chains, and observability) needed to implement a robust RAG pipeline quickly.\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain(\"what is langchain and how RAG is working?\")\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "277f99ea-b044-4590-8853-e887305489b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m sorry, but I don’t understand your request. Could you please clarify what you’d like to know?\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain(\"asfdsa423423adasdasr4@$@EAcASF?\")\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82fa96-c47d-44c5-a76d-46b529ac3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "On mac os\n",
    "=============\n",
    "Use python 3.12\n",
    "ModuleNotFoundError: No module named 'langchain_core.memory'\n",
    "pip uninstall -y langchain langchain-core langchain-community langchainhub\n",
    "pip install -U pip setuptools wheel\n",
    "pip install \"langchain==0.3.7\" \"langchain-core==0.3.17\" \"langchain-community==0.3.7\"\n",
    "\n",
    "|\n",
    "restart kernel \n",
    "|\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"gemma2:2b\")\n",
    "memory = ConversationBufferMemory()\n",
    "chain = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "print(chain.run(\"Hello, I'm using LangChain on macOS!\"))\n",
    "print(chain.run(\"What did I just tell you?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e82c09-481f-4eed-b11b-eb261c2af7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9794ee11-0938-42ed-92e4-61254ccd47ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read python book')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "PromptTemplate.from_template('I likes to read python book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df6d9ff3-2ed2-4139-beb0-20a3d91b69ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7c94dd1-d55f-449f-b9f7-61f63bc5b687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read java book'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obj = PromptTemplate.from_template('I likes to read python book')\n",
    "#obj.format(Userdefined_Key=Value)\n",
    "\n",
    "obj = PromptTemplate.from_template('I likes to read {myvar} book')\n",
    "obj.format(myvar=\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa5aea0-aee5-4a47-92cc-efa8abf824b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read story book'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = PromptTemplate.from_template('I likes to read {myvar} book')\n",
    "obj.format(myvar=\"story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b359c94b-d4f2-4989-86dd-40dd2f7bcfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read GenAI book'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('I likes to read {myvar} book').format(myvar=\"GenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1313742-e3f5-4f25-918a-1af0a1171444",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_obj = PromptTemplate.from_template('''Explain short story about {topic} in {lng} \n",
    "written by {author} released on {my_year}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "964a1713-bbf5-4419-ad48-1f6625de7079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain short story about birds in english \\nwritten by Mr.abc released on 2025'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_obj.format(topic=\"birds\",lng=\"english\",author=\"Mr.abc\",my_year=\"2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f80350-776d-49af-b3d3-90285bc9c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatPromptTemplate\n",
    "- role based \n",
    "user <--> llm\n",
    " |        ===//follow the prompt rule + act as role  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57275649-fa38-4767-8e5b-da6a9100fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#help(ChatPromptTemplate.from_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaec5016-d095-42c8-b7e7-526626185d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Your are helpful AI assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question:{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your are helpful AI assistant\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cda33f49-377e-4660-ad2a-b20691eb9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your are helpful AI assistant\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cec54414-bc23-4087-8068-c110106c2b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Your are helpful AI assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Question:what is langchain?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.format_messages(question=\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d69113d-c050-4f43-b66c-7121eb325183",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**LangChain** is an open‑source Python (and JavaScript) framework that makes it easier to build applications powered by large language models (LLMs).  \\n\\n### Core ideas\\n| Concept | What it does |\\n|---------|--------------|\\n| **Prompt templates** | Re‑usable, parameterized prompts so you can programmatically fill in variables. |\\n| **Chains** | Simple pipelines that connect a prompt (or other component) to an LLM and then to post‑processing steps. |\\n| **Memory** | Keeps track of past interactions so a chatbot can remember conversation history. |\\n| **Agents** | Let an LLM decide which tool or sub‑model to call next (e.g., search, calculator, database). |\\n| **Retrieval‑augmented generation (RAG)** | Combines external data (vector stores, databases, APIs) with LLM output for more factual answers. |\\n| **Integrations** | Built‑in wrappers for many LLM providers (OpenAI, Anthropic, Cohere, **Groq**, etc.) and for vector‑stores, document loaders, APIs, etc. |\\n\\n### Why use it?\\n- **Modularity** – You can swap out components (different LLM, different vector DB) without rewriting the whole app.  \\n- **Productivity** – High‑level abstractions reduce boilerplate code; you can go from “hello‑world” to a full‑featured chatbot or RAG system in minutes.  \\n- **Community** – A large ecosystem of community‑contributed tools, tutorials, and examples.\\n\\nIn short, LangChain provides the building blocks (prompts, chains, memory, agents, retrieval, etc.) that let developers quickly prototype and productionize sophisticated LLM‑driven applications.', additional_kwargs={'reasoning_content': '<tool>search(Langchain framework Groq)</tool>\\n<output>Title: LangChain + Groq: Fast, Composable LLM Apps - GroqDocs\\nURL: https://console.groq.com/docs/langchain\\nContent: Login\\n\\nLog In\\n\\n## 🦜️🔗 LangChain + Groq\\n\\nWhile you could use the Groq SDK directly, LangChain is a framework that makes it easy to build sophisticated applications with LLMs. Combined with Groq API for fast inference speed, you can leverage LangChain components such as: [...] ### Quick Start (3 minutes to hello world)\\n\\n#### 1. Install the package:\\n\\n```\\npip install langchain-groq\\n```\\n\\n#### 2. Set up your API key:\\n\\nbash\\n\\n```\\nexport  GROQ_API_KEY =\"your-groq-api-key\"\\n```\\n\\n#### 3. Create your first LangChain assistant: [...] Challenge: Make the above code your own! Try extending it to include memory with conversation history handling via LangChain to enable users to ask follow-up questions.\\n\\nFor more information on how to build robust, realtime applications with LangChain and Groq, see:\\n\\n Official Documentation: LangChain\\n Groq API Cookbook: Benchmarking a RAG Pipeline with LangChain and LLama\\n Webinar: Build Blazing-Fast LLM Apps with Groq, Langflow, & LangChain\\n\\n### Was this page helpful?\\n\\n#### On this page\\nScore: 0.8885\\n\\nTitle: Groq langchain example: practical guide for developers and SMBs\\nURL: https://www.byteplus.com/en/topic/448585\\nContent: This groq langchain example illustrates how combining Groq’s cutting-edge AI hardware with LangChain’s flexible framework can create scalable, intelligent business tools that meet the demands of modern AI applications. For developers and SMBs eager to harness AI’s potential, this integration offers a pathway to build faster, more efficient solutions that deliver real-time insights and automation. [...] In today’s fast-evolving AI landscape, integrating scalable tech tools like Groq with frameworks such as LangChain can unlock powerful capabilities for developers and small business owners. But how exactly do you combine these intelligent business tools effectively? This groq langchain example will walk you through practical steps to leverage Groq’s high-performance AI hardware alongside LangChain’s flexible framework for building language model applications. Whether you’re a developer looking [...] LangChain, on the other hand, is an open-source framework that simplifies building applications powered by large language models (LLMs). It provides modular components for chaining prompts, managing memory, and integrating external data sources, enabling developers to build intelligent business tools that are both scalable and customizable.\\nScore: 0.8580\\n\\nTitle: Unlocking the Power of Groq\\'s LLM with LangChain - Medium\\nURL: https://medium.com/@priyanka_neogi/unlocking-the-power-of-groqs-llm-with-langchain-f29e926bf406\\nContent: LangChain: A powerful open-source framework that simplifies the creation of applications powered by language models. It provides handy abstractions like prompt templates and chains to streamline working with different LLM providers.\\n Groq’s ChatGroq: A wrapper to connect LangChain with Groq’s language models, including LLaMA 3 variants. This allows you to access Groq’s advanced AI models using LangChain’s interface, just like you would with OpenAI or Anthropic models. [...] This simple yet elegant integration shows how to build AI-powered code generation tools with minimal effort. You can expand this framework to create chatbots, question-answering systems, or intelligent assistants that understand your custom instructions.\\n\\nThe beauty of LangChain is its abstraction — no need to manually format API calls or parse raw outputs. And Groq’s powerful LLM models provide high-quality responses with impressive speed. [...] Today, I’m going to walk you through a simple yet effective example that integrates Groq’s LLM with the LangChain framework to automatically generate Python code for any given programming task.\\n\\nBy the end of this post, you will understand the key components of the integration, how they work together, and the purpose of each line of code — explained clearly in both layman’s and technical terms.\\n\\n## What Is LangChain and Groq?\\nScore: 0.8522\\n\\nTitle: Groq - Install LangChain\\nURL: https://python.langchain.com/docs/integrations/providers/groq/\\nContent: 🦜️🔗 LangChain\\n🦜️🔗 LangChain\\nOpen on GitHub\\n\\n# Groq\\n\\nGroq developed the world\\'s first Language Processing Unit™, or `LPU`.\\nThe `Groq LPU` has a deterministic, single core streaming architecture that sets the standard\\nfor GenAI inference speed with predictable and repeatable performance for any given workload.\\n\\n`LPU`\\n`Groq LPU`\\n\\nBeyond the architecture, `Groq` software is designed to empower developers like you with\\nthe tools you need to create innovative, powerful AI applications.\\n\\n`Groq` [...] With Groq as your engine, you can:\\n\\n## Installation and Setup\\u200b\\n\\nInstall the integration package:\\n\\n`pip install langchain-groq`\\n\\nRequest an API key and set it as an environment variable:\\n\\n`export GROQ_API_KEY=gsk_...`\\n\\n## Chat models\\u200b\\n\\nSee a usage example.\\n\\n`from langchain_groq import ChatGroq`\\nScore: 0.7950\\n\\nTitle: ChatGroq - LangChain.js\\nURL: https://js.langchain.com/docs/integrations/chat/groq/\\nContent: These docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docs\\n\\n# ChatGroq\\n\\nGroq is a company that offers fast AI inference, powered by LPU™ AI inference technology which delivers fast, affordable, and energy efficient AI.\\n\\nThis will help you getting started with ChatGroq chat models. For detailed documentation of all ChatGroq features and configurations head to the API reference.\\n\\n## Overview\\u200b\\n\\n### Integration details\\u200b [...] To access ChatGroq models you’ll need to create a Groq account, get an API key, and install the `@langchain/groq` integration package.\\n\\n### Credentials\\u200b\\n\\nIn order to use the Groq API you’ll need an API key. You can sign up for a Groq account and create an API key here. Then, you can set the API key as an environment variable in your terminal:\\n\\n```\\nexport GROQ_API_KEY=\"your-api-key\" export  GROQ_API_KEY =\"your-api-key\"\\n``` [...] ```\\nimport { ChatGroq } from \"@langchain/groq\"; import  { ChatGroq }  from  \"@langchain/groq\";  const llm = new ChatGroq({ const  llm =  new  ChatGroq({ model: \"llama-3.3-70b-versatile\",  model:  \"llama-3.3-70b-versatile\",  temperature: 0,  temperature:  0,  maxTokens: undefined,  maxTokens:  undefined,  maxRetries: 2,  maxRetries:  2,  // other params...  // other params... }); });\\n```\\n\\n## Invocation\\u200b\\nScore: 0.7619\\n\\nTitle: How to Implement Groq Embeddings in LangChain - BytePlus\\nURL: https://www.byteplus.com/en/topic/448578\\nContent: Implementing Groq embeddings in LangChain is more than a technical exercise—it\\'s a strategic move towards building more intelligent, responsive AI systems. By following this comprehensive guide, developers can unlock new dimensions of language understanding and processing.\\n\\nReady to Transform Your AI Projects?\\n\\nStart experimenting with Groq embeddings in LangChain today. Visit the official Groq documentation and LangChain resources to dive deeper into this exciting technology. [...] In the rapidly evolving landscape of AI development, integrating advanced embedding technologies has become crucial for developers seeking to build intelligent, scalable solutions. If you\\'re looking to supercharge your language models with cutting-edge embedding capabilities, implementing Groq embeddings in LangChain offers a powerful approach to enhancing your AI applications.\\n\\n## Understanding groq embeddings and langchain integration [...] Implementing Groq embeddings in LangChain requires a strategic approach that combines technical precision with practical implementation. Here\\'s a comprehensive walkthrough to help you navigate the integration process:\\n\\n### Prerequisite setup\\n\\nBefore diving into the implementation, ensure you have the following:\\n\\n### Installation and configuration\\n\\nStart by installing the necessary libraries:\\n\\n`pip install langchain groq-python`\\n\\n### Code implementation example\\nScore: 0.7592\\n\\nTitle: LangChain + Groq + Spider = (Integration Guide)\\nURL: https://spider.cloud/guides/langchain-groq\\nContent: LangChain is a powerful LLM orchestration API framework, but in this example we\\'ll use it more simply to put together our prompt and run a chain.\\nScore: 0.7533\\n\\nTitle: How to Access Groq AI Models Using LangChain - YouTube\\nURL: https://www.youtube.com/watch?v=oFAetlRRTNU\\nContent: ### Playlist Description:  \\nUnlock the power of Groq AI models with LangChain! 🚀 In this tutorial, we’ll guide you through integrating Groq AI models into your LangChain-based applications. Whether you\\'re building chatbots, AI assistants, or NLP applications, this hands-on guide covers everything from setting up Groq API, LangChain configuration, token handling, and executing AI-powered queries efficiently. [...] Groq AI, LangChain tutorial, access Groq models, Groq AI with LangChain, LangChain API, AI model integration, LangChain hands-on, AI chatbot development, NLP with LangChain, generative AI, LangChain guide, LangChain framework, AI-powered applications, Groq AI setup, LangChain model access, AI model API, Groq AI API, LangChain workflow, AI model deployment, LangChain for AI, Groq AI use cases, Groq AI implementation, AI in Python, LangChain libraries, LangChain Groq connection, AI development, [...] Groq AI, LangChain tutorial, access Groq models, Groq AI with LangChain, LangChain API, AI model integration, LangChain hands-on, AI chatbot development, NLP with LangChain, generative AI, LangChain guide, LangChain framework, AI-powered applications, Groq AI setup, LangChain model access, AI model API, Groq AI API, LangChain workflow, AI model deployment, LangChain for AI, Groq AI use cases, Groq AI implementation, AI in Python, LangChain libraries, LangChain Groq connection, AI development,\\nScore: 0.7514\\n\\nTitle: Groq - Docs by LangChain\\nURL: https://docs.langchain.com/oss/python/integrations/providers/groq\\nContent: ## \\u200b Installation and Setup\\n\\nInstall the integration package:\\n\\nCopy\\n\\n```\\npip install langchain-groq\\n\\n```\\n\\nRequest an API key and set it as an environment variable:\\n\\nCopy\\n\\n```\\nexport GROQ_API_KEY=gsk_...\\n\\n```\\n\\n## \\u200b Chat models\\n\\nSee a usage example.\\n\\nCopy\\n\\n```\\nfrom langchain_groq import ChatGroq\\n\\n```\\n\\nWas this page helpful?\\n\\nAssistant\\n\\nResponses are generated using AI and may contain mistakes. [...] > Groq developed the world’s first Language Processing Unit™, or `LPU`.\\n> The `Groq LPU` has a deterministic, single core streaming architecture that sets the standard\\n> for GenAI inference speed with predictable and repeatable performance for any given workload.\\n> Beyond the architecture, `Groq` software is designed to empower developers like you with\\n> the tools you need to create innovative, powerful AI applications.\\n> With Groq as your engine, you can:\\n> [...] Docs by LangChain home page\\n\\nLangChain\\nLangGraph\\nIntegrations\\nLearn\\nReference\\nContributing\\n\\nLangChain\\nLangGraph\\nIntegrations\\nLearn\\nReference\\nContributing\\n\\n- GitHub\\n- Forum\\n\\nOn this page\\n\\n Installation and Setup\\n Chat models\\n\\n# Groq\\nScore: 0.7376\\n\\nTitle: Building a RAG App with LangChain, Groq, and Streamlit - Medium\\nURL: https://medium.com/@bharatchaudhury/from-zero-to-lightning-fast-building-a-rag-app-with-langchain-groq-and-streamlit-00a96a616f4d\\nContent: This project serves as a fantastic demonstration of how to build a responsive, intelligent, and efficient RAG application. The synergy between LangChain’s powerful orchestration capabilities, Groq’s blazing-fast inference, and Streamlit’s ease of use creates a seamless and highly performant AI experience.\\n\\nHere are some of my exciting ideas for extending and improving this project: [...] LangChain: It’s an indispensable framework for developing applications powered by large language models. It simplifies the complex process of building LLM workflows from loading and splitting documents to managing retrieval and crafting effective prompts. [...] Groq: This is where the “lightning-fast” part comes in ! Groq provides a Language Processing Unit (LPU) inference engine that offers incredibly high-speed inference for large language models. This means your RAG queries get answered almost instantly, providing a superior user experience compared to traditional CPU/GPU-based inference.That sucks on my machine !\\nScore: 0.7360\\n\\nTitle: Mixture of Agents Powered by Groq using Langchain LCEL - Tutorials\\nURL: https://community.groq.com/t/mixture-of-agents-powered-by-groq-using-langchain-lcel/110\\nContent: This notebook demonstrates the implementation of a Mixture of Agents (MoA) architecture using Langchain and Groq. The MoA approach combines multiple open source models to produce responses that are on par or better than SOTA proprietary models like GPT4.\\n\\nThis tutorial will walk you through how to:\\n\\n1. Set up the environment and dependencies.\\n2. Create helper functions.\\n3. Configure and build the Mixture of Agents pipeline.\\n4. Chat with the Agent.\\n\\n825×461 21.4 KB [...] In this notebook we demonstrated how to build a fairly complex agentic workflow using Groq\\'s fast AI inference. MoA and other agentic workflows offer significant advantages in working with Large Language Models (LLMs). By enabling LLMs to \"think,\" refine their responses, and break down complex tasks, these approaches enhance accuracy and problem-solving capabilities. Moreover, they present a cost-effective solution by allowing the use of smaller, open-source models in combination, even when [...] ```\\nfrom typing import Dict, Optional, Generator  \\nfrom textwrap import dedent  \\n  \\nfrom langchain_groq import ChatGroq  \\nfrom langchain.memory import ConversationBufferMemory  \\nfrom langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableSerializable  \\nfrom langchain_core.output_parsers import StrOutputParser  \\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder  \\n  \\n# Helper method to create an LCEL chain  \\ndef create_agent(\\nScore: 0.7154\\n\\n</output>'}, response_metadata={'token_usage': {'completion_tokens': 593, 'prompt_tokens': 3575, 'total_tokens': 4168, 'completion_time': 1.43396, 'prompt_time': 0.168494, 'queue_time': 0.096867, 'total_time': 1.602455}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2021f341-ff96-4ade-81cf-cd791b4cf464-0', usage_metadata={'input_tokens': 3575, 'output_tokens': 593, 'total_tokens': 4168})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_obj = ChatGroq(model=\"groq/compound-mini\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "#chat_prompt = prompt rules \n",
    "#llm_obj = llm-model\n",
    "\n",
    "chain = chat_prompt|llm_obj\n",
    "chain.invoke({'question':'what is langchain?'}) # chainObject.invoke({'placeholder':'userQuery'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28f9c294-3790-4af8-9fe0-c62e85c9136c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are five important points to understand about Generative AI (Gen\\u202fAI):\\n\\n1. **Creates New Content Across Modalities**  \\n   - Gen\\u202fAI can generate text, images, audio, video, code, and even scientific hypotheses, re‑using patterns it learned from massive datasets. (AWS)\\n\\n2. **Accelerates Product Development & Creativity**  \\n   - By quickly producing drafts, prototypes, or design variations, it shortens time‑to‑market and lets humans focus on higher‑level, strategic work. (Oracle)\\n\\n3. **Drives Operational Efficiency & Cost Savings**  \\n   - Automated routine tasks, faster data analysis, and reduced error rates lower labor costs and improve productivity—often cutting task duration roughly in half. (Oracle)\\n\\n4. **Emerging “Agentic AI” and Autonomous Agents**  \\n   - The next wave (2025 onward) involves AI agents that can act with minimal human supervision, handling end‑to‑end workflows such as customer support, data retrieval, and decision support. (Synoptek)\\n\\n5. **Adoption Challenges: Trust, Security, and Talent**  \\n   - Hallucinations, data‑privacy concerns, and a shortage of skilled AI talent are the main hurdles businesses must address with guardrails, human oversight, and robust security practices. (AWS, Oracle, AmplifAI)', additional_kwargs={'reasoning_content': \"<tool>search(5 important points about Gen AI)</tool>\\n<output>Title: GenAI Trends 2025: 5 Key Developments to Watch out for - Synoptek\\nURL: https://synoptek.com/insights/thought-leadership/data-insights/genai-trends-key-developments-to-watch-out-for/\\nContent: As we enter 2025, Generative AI will continue transforming industries, offering unprecedented opportunities for innovation and efficiency. From transforming customer experiences to driving automation in various sectors, GenAI’s scope is expanding rapidly. As this technology matures, businesses are moving past early-stage prototypes. They are now looking for real-world applications that can deliver measurable impact. [...] Here are the top 5 GenAI trends to watch in 2025. These trends highlight the key developments that will shape the future of this transformative technology.\\n\\n### 1. Agentic AI\\n\\n2025 will witness the next frontier of Generative AI innovation, agentic AI, taking over the world. These intelligent agents will operate autonomously, requiring little to no human input to carry out tasks. [...] Artificial Intelligence undeniably drives the future, and 2025 will set the stage for the next wave of transformation. The coming year will be a turning point for businesses embracing GenAI, reshaping how they operate, compete, and innovate.\\n\\nWith breakthroughs in agentic AI, retrieval-augmented generation, self-training models, inference scaling, and ethical AI shaping the future of business, these trends will enhance operational efficiencies like never before.\\nScore: 0.7240\\n\\nTitle: What Is Generative AI (GenAI)? How Does It Work? - Oracle\\nURL: https://www.oracle.com/artificial-intelligence/generative-ai/what-is-generative-ai/\\nContent: Faster product launches: Generative AI can quickly produce product prototypes and first drafts, help fine-tune works in progress, and test/troubleshoot existing projects to find improvements much faster than previously possible.\\n Quality control: An enterprise-specific, specialized generative AI model is likely to expose gaps and inconsistencies in the user manuals, videos, and other content that a business presents to the public. [...] ## Why Is Generative AI Important?\\n\\nA useful way to understand the importance of generative AI is to think of it as a calculator for open-ended, creative content. Like a calculator automates routine and mundane math, freeing up a person to focus on higher-level tasks, generative AI has the potential to automate the more routine and mundane subtasks that make up much of knowledge work, freeing people to focus on the higher-level parts of the job. [...] Reduced costs: Because of their speed, generative AI tools reduce the cost to complete processes, and if it takes half the time to do a task, the task costs half as much as it otherwise would. In addition, generative AI can minimize errors, eliminate downtime, and identify redundancies and other costly inefficiencies. There is an offset, however: Because of generative AI’s tendency to hallucinate, human oversight and quality control is still necessary. But human-AI collaborations are expected\\nScore: 0.6887\\n\\nTitle: GenAI In-Depth: The Science and Capabilities of GenAI - UM-GPT\\nURL: https://genai.umich.edu/in-depth/science-and-capabilities\\nContent: First, advances in deep learning techniques, specifically transformer architectures, have enabled more powerful and efficient modeling of complex relationships in the data. Second, the availability of vast and diverse datasets, encompassing extensive text corpora and video repositories, has significantly contributed to the quality and diversity of generated outputs. Third, the substantial growth in computational power has played a crucial role in enabling the training and deployment of [...] The field of Generative AI has experienced exponential advancements in recent years, demonstrating remarkable progress across diverse modalities such as text, images, sound, and more. These advancements can be primarily attributed to the three main factors: methods, data, and scale of computation. [...] GenAI systems often produce misinformative statements and make unsupported claims. Their inability to provide a confidence level for the information they provide makes it difficult to determine when to trust these models. Further, these AI systems can be exploited to generate false or misleading information. The resulting misinformative content can have significant negative implications for trust, credibility, and the manipulation of public opinion.\\n   Lack of transparency\\nScore: 0.6560\\n\\nTitle: 60+ Generative AI Statistics You Need to Know in 2025 - AmplifAI\\nURL: https://www.amplifai.com/blog/generative-ai-statistics\\nContent: In a 2024 Mckinsey survey 65% of respondents reported their organizations are regularly using gen AI, a 49% increase from 2023. Cite source\\n\\n3\\n\\n31% of North American companies are categorized as AI leaders, while 16% fall under the category of AI laggards. Cite source\\n\\n4\\n\\n Compared to 2023, there has been a $22 billion increase in the global gen AI market. Cite source\\n\\n5\\n\\n29% of AI leaders implement the technology in less than three months, versus laggards at only 6%. Cite source\\n\\n6 [...] The latest Gen AI statistics show that 47% of US executives see Gen AI boosting productivity (54), while content creation (40%) and customer interaction analysis (31%) emerge as the top use cases in contact centers (47).\\n\\nThe transformation goes deeper than efficiency gains.\\n\\nOne out of three decision-makers report Gen AI enables better prediction of customer needs and more personalized experiences (13).\\n\\n### Latest Generative AI Statistics on Impact to Businesses\\n\\n49 [...] This divide is most visible in customer service. While 59% of companies see Gen AI transforming customer interactions (27), two major hurdles stand in their way: 75% of customers worry about data security (56), and 45% of businesses lack the talent to implement AI effectively (55).\\n\\nCompanies solving the Gen AI adoption challenges first are gaining a significant head start over their competitors.\\nScore: 0.6531\\n\\nTitle: What is Generative AI? - Gen AI Explained - AWS - Updated 2025\\nURL: https://aws.amazon.com/what-is/generative-ai/\\nContent: Generative artificial intelligence (generative AI) is a type of AI that can create new content and ideas, including conversations, stories, images, videos, and music. It can learn human language, programming languages, art, chemistry, biology, or any complex subject matter. It reuses what it knows to solve new problems. For example, it can learn English vocabulary and create a poem from the words it processes. Your organization can use generative AI for various purposes, like chatbots, media [...] ### Implement security\\n\\nImplement guardrails so your generative AI applications don't allow inadvertent unauthorized access to sensitive data. Involve security teams from the start so all aspects can be considered from the beginning. For example, you may have to mask data and remove personally identifiable information (PII) before you train any models on internal data.\\n\\n### Test extensively [...] Generative AI algorithms can explore and analyze complex data in new ways, allowing researchers to discover new trends and patterns that may not be otherwise apparent. These algorithms can summarize content, outline multiple solution paths, brainstorm ideas, and create detailed documentation from research notes. This is why generative AI drastically enhances research and innovation.\\nScore: 0.6437\\n\\n</output>\"}, response_metadata={'token_usage': {'completion_tokens': 419, 'prompt_tokens': 2013, 'total_tokens': 2432, 'completion_time': 0.965861, 'prompt_time': 0.246787, 'queue_time': 1.134527, 'total_time': 1.212648}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9451eaa8-e4d7-47e1-b3cc-e113eb4cca8b-0', usage_metadata={'input_tokens': 2013, 'output_tokens': 419, 'total_tokens': 2432})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'give me 5 import points about Gen AI?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff16447b-715f-4c6e-b0f2-5ab6d152c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"don't know\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d88bda4-644e-4e70-a113-fd22ce733836",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**LangChain** is an open‑source framework that makes it easy to build applications powered by large language models (LLMs) such as GPT‑4, Claude, Gemini, etc.  \\n\\nKey points:\\n\\n| Aspect | What LangChain Provides |\\n|--------|--------------------------|\\n| **Unified API** | A standard interface for LLMs, embedding models, vector stores, and many other AI services, so you can swap models or providers with minimal code changes. |\\n| **Modular Building Blocks** | • **Chains** – sequences of prompts, model calls, and post‑processing steps.<br>• **Agents** – LLMs that can decide which tool or action to take next (e.g., search the web, run code).<br>• **Memory** – persistent context across interactions.<br>• **Retrieval** – connect LLMs to external data via vector databases, search APIs, etc. |\\n| **Tool Integration** | Connects LLMs to external tools (databases, APIs, calculators, web browsers, etc.) so the model can act on real‑world information. |\\n| **Languages** | Available for Python and JavaScript/TypeScript, with many community‑maintained integrations (OpenAI, Anthropic, Hugging Face, Azure, AWS, etc.). |\\n| **Ecosystem** | Includes related projects like **LangServe** (host chains as APIs), **LangSmith** (monitoring/debugging), and **LangGraph** (multi‑agent workflow orchestration). |\\n| **Use Cases** | Chatbots, question‑answering over private documents, summarization, code generation, data analysis, autonomous agents, and any workflow that needs LLMs combined with external data or actions. |\\n\\nIn short, LangChain abstracts away the boiler‑plate of wiring LLMs to tools, memory, and data sources, letting developers focus on the high‑level logic of their AI‑driven applications.', additional_kwargs={'reasoning_content': '<tool>search(what is LangChain)</tool>\\n<output>Title: LangChain - Wikipedia\\nURL: https://en.wikipedia.org/wiki/LangChain\\nContent: Image 7Free and open-source software portal\\n\\nLangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.(\\n\\nHistory\\n\\n[edit] [...] In the third quarter of 2023, the LangChain Expression Language (LCEL) was introduced, which provides a declarative way to define chains of actions.(\\n\\nIn October 2023 LangChain introduced LangServe, a deployment tool to host LCEL code as a production-ready API.( [...] | Image 6: 🦜️🔗, the parrot and chain emojis |\\n| Developer | Harrison Chase |\\n| Initial release | October 2022 |\\n|  |\\n| Stable release | 0.1.16 / 11 April 2024; 18 months ago(11 April 2024) |\\n|  |\\n| Repository \"Repository (version control)\") | github.com/langchain-ai/langchain |\\n| Written in | Python \"Python (programming language)\") and JavaScript |\\n| Type | Software framework for large language model application development |\\n| License | MIT License |\\n| Website | LangChain.com |\\nScore: 0.9476\\n\\nTitle: Introduction to LangChain - GeeksforGeeks\\nURL: https://www.geeksforgeeks.org/artificial-intelligence/introduction-to-langchain/\\nContent: # Introduction to LangChain\\n\\nLast Updated : \\n25 Aug, 2025\\n\\nSuggest changes\\n\\n12 Likes\\n\\nLangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a standard interface for integrating with other tools and end-to-end chains for common applications. It helps AI developers connect LLMs such as GPT-4 with external data and computation. This framework comes for both Python and JavaScript.\\n\\nKey benefits include: [...] Vector database plays a key role in tasks like document retrieval, knowledge base integration or context-based search providing the model with dynamic, real-time data to enhance responses.\\n\\n5. Models: LangChain is model-agnostic meaning it can integrate with different LLMs such as OpenAI\\'s GPT, Hugging Face models, DeepSeek R1 and more. This flexibility allows developers to choose the best model for their use case while benefiting from LangChain’s architecture. [...] 2. Prompt Management: LangChain facilitates managing and customizing prompts passed to the LLM. Developers can use PromptTemplates to define how inputs and outputs are formatted before being passed to the model. It also simplifies tasks like handling dynamic variables and prompt engineering, making it easier to control the LLM\\'s behavior.\\nScore: 0.9415\\n\\nTitle: What Is LangChain and How to Use It: A Guide - TechTarget\\nURL: https://www.techtarget.com/searchenterpriseai/definition/LangChain\\nContent: Definition\\n\\n# What is LangChain and how to use it: A guide\\n\\nBy\\n\\n Kinza Yasar, Technical Writer\\n Cameron Hashemi-Pour, Former Site Editor\\n\\nPublished: Jan 02, 2025\\n\\nLangChain is an open source framework that enables software developers working with artificial intelligence (AI) and its machine learning subset to combine large language models with other external components to develop LLM-powered applications. [...] LangChain is a framework that simplifies the process of creating generative AI application interfaces. Developers working on these types of interfaces use various tools to create advanced NLP apps; LangChain streamlines this process. For example, LLMs must access large volumes of big data, so LangChain organizes these large quantities of data so that they can be accessed with ease.\\n\\nThis article is part of\\n\\n### What is GenAI? Generative AI explained [...] Simplified development. LangChain offers a standardized interface that enables developers to easily switch between different LLMs, streamline workflows and reduce integration complexity. For example, they can switch between LLMs from providers such as OpenAI or Hugging Face with minimal code changes.\\nScore: 0.9351\\n\\nTitle: What Is LangChain? Examples and definition | Google Cloud\\nURL: https://cloud.google.com/use-cases/langchain\\nContent: LangChain is an open-source orchestration framework that simplifies building applications with large language models (LLMs). It provides tools and components to connect LLMs with various data sources, enabling the creation of complex, multi-step workflows. [...] Available as libraries in Python and JavaScript, LangChain helps developers enhance LLM capabilities beyond text generation by linking them to external data and computation. This helps facilitate the development of advanced AI applications like intelligent chatbots, sophisticated question-answering systems, and automated data analysis tools.\\n\\nGet started for free [...] Speech-to-Text Speech recognition and transcription across 125 languages.\\n\\n   Text-to-Speech Speech synthesis in 220+ voices and 40+ languages.\\n\\n   Translation AI Language detection, translation, and glossary support.\\n\\n   Gemini Enterprise Secure platform to discover, create, run, and govern AI agents.\\n\\n   Vision AI Custom and pre-trained models to detect emotion, text, and more.\\n\\n   Contact Center as a Service Omnichannel contact center solution that is native to the cloud.\\nScore: 0.9228\\n\\nTitle: What is LangChain? - AWS\\nURL: https://aws.amazon.com/what-is/langchain/\\nContent: LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queries—for example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. For example, developers can use LangChain components to build new [...] Using Amazon Bedrock, Amazon Kendra, Amazon SageMaker JumpStart, LangChain, and your LLMs, you can build highly-accurate generative artificial intelligence (generative AI) applications on enterprise data. LangChain is the interface that ties these components together: [...] Some conversational language model applications refine their responses with information recalled from past interactions. LangChain allows developers to include memory capabilities in their systems. It supports:\\n\\n Simple memory systems that recall the most recent conversations.\\n Complex memory structures that analyze historical messages to return the most relevant results.\\n\\n### Callbacks\\nScore: 0.9223\\n\\nTitle: Introduction | 🦜️   Langchain\\nURL: https://js.langchain.com/docs/introduction/\\nContent: These docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docs\\n\\n# Introduction\\n\\nLangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle: [...] `@langchain/core`: Base abstractions and LangChain Expression Language.\\n `@langchain/community`: Third party integrations.\\n  + Partner packages (e.g. `@langchain/openai`, `@langchain/anthropic`, etc.): Some integrations have been further split into their own lightweight packages that only depend on `@langchain/core`.\\n `langchain`: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture. [...] LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.\\n\\n### Contributing\\u200b\\n\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n\\n#### Was this page helpful?\\n\\n  \\n\\n#### You can also leave detailed feedbackon GitHub.\\nScore: 0.9190\\n\\nTitle: What is LangChain? - YouTube\\nURL: https://www.youtube.com/watch?v=1bUy-1hGZpI\\nContent: Now stop me if you\\'ve heard this one before, but there are a lot of large language models available today, and they have their own capabilities and specialties. What if I prefer to use one LLM to interpret some user queries in my business application, but a whole other LLM to author a response to those queries? Well, that scenario is exactly what the LangChain caters to. Langchain as an open source orchestration framework for the development of applications that use large language models. And [...] the right workflows, LangChain\\'s agent modules can use an LLM to autonomously determine the next steps, and then take the action that it needs to complete that step using something called RPA, or robotic process automation. LangChain is open source and free to use. There are also related frameworks like LangServe for creating chains as REST APIs and then LangSmith, which provides tools to monitor, evaluate and debug applications. Essentially LangChain\\'s tools and APIs simplify the process of [...] LangChain hype train has slightly cooled a little bit, there\\'s plenty of utility here. So let\\'s take a look at its components. So what makes up long chain? Well, LangChain streamlines the programing of LLM applications through something called abstractions. Now, what do I mean by that? Well, your thermostat that allows you to control the temperature in your home without needing to understand all the complex circuitry that this entails. We just set the temperature. That\\'s an abstraction. So\\nScore: 0.8791\\n\\nTitle: What Is LangChain? | IBM\\nURL: https://www.ibm.com/think/topics/langchain\\nContent: ### LangGraph\\n\\nLangGraph, created by LangChain,is an open source AI agent framework that supports multi-agent orchestration and enables developers to buildagentic workflowswhere different agents interact, specialize and collaborate. [...] LangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that [...] Despite their heralded power and versatility, LLMs have important limitations: namely, a lack of up-to-date information, a lack of domain-specific expertise and a general difficulty with math.\\n\\nLangChain _tools_are a set of functions that empower LangChain agents to interact with real-world information in order to expand or improve the services it can provide. Examples of prominent pre-built LangChain tools include:\\nScore: 0.8677\\n\\nTitle: introduction - Install LangChain\\nURL: https://python.langchain.com/docs/introduction/\\nContent: Image 3: Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\\nLangChain implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. See the integrations page for more.\\n\\nSelect chat model:\\n\\nGoogle Gemini▾ [...] `langchain-core`: Base abstractions for chat models and other components.\\n   Integration packages (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n   `langchain`: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n   `langchain-community`: Third-party integrations that are community maintained. [...] LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. If you\\'re looking to get up and running quickly with chat models, vector stores, or other LangChain components from a specific provider, check out our growing list of integrations.\\n\\n### API reference\\u200b\\n\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\nEcosystem\\u200b\\n\\n### 🦜🛠️ LangSmith\\u200b\\nScore: 0.8393\\n\\nTitle: What is Langchain and why should I care as a developer? - Medium\\nURL: https://medium.com/around-the-prompt/what-is-langchain-and-why-should-i-care-as-a-developer-b2d952c42b28\\nContent: You could also spin up a vector database (special database just for Embeddings which are a numerical way of representing the meaning of text) but this often takes a bit of work. Langchain has a memory module which provides plug and play access to multiple data store which allow you to save the message history of a conversation automatically further reducing the friction to create a chatbot for example. [...] Langchain 🦜 is one of the fastest growing open source projects in history, in large part due to the explosion of interest in LLM’s.\\n\\nThis post explores some of the cool thing that langchain helps developers do from a 30,000 foot overview. It was written for my own benefit as I explored the framework and I hope it helps you if you are also curios where langchain might be useful. [...] ## Agents in Langchain 🤖\\n\\nOne of the hottest ideas in the large language models space right now is the idea of agents. By using language models, you can essentially recreate a programmatic entity that has goals and tasks it can execute.\\n\\nLangchain makes creating agents using large language models simple through their agents API. Developers can use OpenAI functions or other means of executing tasks to enable to language model to take actions.\\nScore: 0.7518\\n\\n</output>'}, response_metadata={'token_usage': {'completion_tokens': 473, 'prompt_tokens': 3273, 'total_tokens': 3746, 'completion_time': 1.113372, 'prompt_time': 0.225652, 'queue_time': 0.108506, 'total_time': 1.339024}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--eb3ac92c-ac2d-47e4-ae4f-9f35b403af7d-0', usage_metadata={'input_tokens': 3273, 'output_tokens': 473, 'total_tokens': 3746})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt|llm_obj\n",
    "llm_obj = ChatGroq(model=\"groq/compound-mini\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "chain.invoke({'question':'what is langchain?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6e5d0ad-a806-4755-9440-ea292bb98f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = chain.invoke({'question':'what is python?'})\n",
    "r2 = chain.invoke({'question':'ssadfsasfsdsd?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e2f4098-c98e-4a0f-93c2-a688341c805b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Python is a high‑level, interpreted programming language known for its readability and simplicity. It uses clear, English‑like syntax, supports multiple programming paradigms (procedural, object‑oriented, functional), and comes with a vast standard library plus a rich ecosystem of third‑party packages. Because it’s easy to learn and works on virtually any platform, Python is widely used for web development, data analysis, scientific computing, automation, artificial intelligence, and many other fields.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 448, 'prompt_tokens': 459, 'total_tokens': 907, 'completion_time': 0.928864, 'prompt_time': 0.033173, 'queue_time': 0.107997, 'total_time': 0.962038}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--0aacd01c-9621-4760-8dcb-3d50d604c2d8-0' usage_metadata={'input_tokens': 459, 'output_tokens': 448, 'total_tokens': 907}\n"
     ]
    }
   ],
   "source": [
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e88a55d1-2b0f-4f05-ae49-ee380064179a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I’m not sure I understand your question—could you please clarify or provide more details?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 465, 'total_tokens': 532, 'completion_time': 0.152739, 'prompt_time': 0.034303, 'queue_time': 0.320063, 'total_time': 0.187041}, 'model_name': 'groq/compound-mini', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--5b4dd7a8-091e-4271-ae89-3f7320ff7a68-0' usage_metadata={'input_tokens': 465, 'output_tokens': 67, 'total_tokens': 532}\n"
     ]
    }
   ],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ae4ad7b-08ed-4be7-9a80-a683a0b99922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7accc02a-db3c-4a09-95ef-0f55c0464a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_53412\\2052176731.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  my_llm_obj = Ollama(model=\"gemma2:2b\")\n"
     ]
    }
   ],
   "source": [
    "my_llm_obj = Ollama(model=\"gemma2:2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61f0d3bc-dd63-4c16-a3ae-b32ecca67cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chain = chat_prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1de2b94-068c-4452-8daa-4d38193a9c9a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GenAI stands for **Generative Artificial Intelligence**. \\n\\nHere\\'s a simple explanation:\\n\\n**Imagine AI that can create things, like:**\\n\\n* **Text:**  Writing poems, scripts, articles, and even code.\\n* **Images:**  Generating realistic photographs, paintings, or digital art.\\n* **Audio:** Creating music, sound effects, or voice-over recordings. \\n* **Video:** Making short films, animations, or even editing existing video footage.\\n\\n**How it works:**\\nGenAI uses powerful algorithms and massive datasets to learn patterns and create new things based on this learning. Think of it like an artist who has studied countless paintings but can now paint their own unique masterpiece.  \\n\\n**Examples of GenAI:**\\n\\n* **DALL-E 2:** Can generate images from text prompts (like \"a fluffy cat in a spaceship\").\\n* **ChatGPT:** Can write creative stories, answer questions, and even summarize complex texts.\\n* **Midjourney:** Creates stunningly beautiful art pieces based on text descriptions.\\n\\n\\nGenAI is still evolving rapidly, but it holds immense potential to change many aspects of our lives, from creating new forms of art and entertainment to advancing scientific research and problem-solving. \\n\\n\\n\\nLet me know if you\\'d like more information or have any other questions! 😊 \\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'what is genAI?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00f15649-6449-460e-9cdc-dabbbb21cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"Say don't know\"),(\"user\",'Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "chain = prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2be6433f-4a3b-4222-9a62-3bc193aae29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Say: \"I don\\'t know.\" \\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'xyzdfsfsfsa?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d008b3e-e7ca-44c4-861a-98d8f9558a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't know. \\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'what is python'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "226f4e55-7ba5-42f2-b8cb-0409d419dac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know. \\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'what is genAI'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0dfefb4-b386-415d-b071-5263794feaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"\"),(\"user\",'Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "chain = prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc14c45f-cc52-455a-95a7-82d0f784f141",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's break down what Gen AI means:\\n\\n**Gen AI stands for Generative Artificial Intelligence.**  \\n\\nThink of it this way:\\n\\n* **Traditional AI:**  AI systems that focus on analyzing and understanding existing data (like images, text, numbers) to perform tasks. Think about spam filters or image recognition apps.\\n* **Generative AI:** AI systems that can create new things!  They take in input from various sources (text prompts, images, code) and generate outputs like: \\n    * **Text:** Poems, scripts, articles, even code\\n    * **Images:** Paintings, photos, realistic renderings\\n    * **Audio:** Music, voiceovers, sound effects \\n    * **Video:**  Short movies, animated scenes\\n\\n**Here are some key points about Gen AI:**\\n\\n* **It's based on Machine Learning (ML):**  Gen AI uses complex algorithms to learn patterns from massive amounts of data and then use this knowledge to create new things. \\n* **Creative Powerhouse:** It can mimic human creativity, but with its own unique style and approach.\\n* **Evolving Fast:** This field is exploding in terms of development! New tools and capabilities are emerging constantly.\\n\\n**Here's an analogy:** Imagine a painter learning from thousands of paintings to create their own masterpieces. That's what Gen AI does – it learns from vast datasets and uses that knowledge to create something new.\\n\\n\\nLet me know if you have any other questions! \\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'what is genAI'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ad653-35e1-4a9d-a9e1-89cd23747aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
